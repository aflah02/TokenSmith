{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b5b205",
   "metadata": {},
   "source": [
    "# Dataset Inspection Tutorial\n",
    "\n",
    "This tutorial demonstrates how to inspect samples from your tokenized dataset using TokenSmith's inspection functionality. We'll build on the setup from the first tutorial to explore individual samples and batches.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Complete the first tutorial (01_basic_setup.ipynb)\n",
    "- Have a tokenized dataset ready with batch info generated\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to inspect individual samples by ID\n",
    "- How to inspect batches of samples\n",
    "- Understanding document details and metadata\n",
    "- Working with tokenized vs detokenized content\n",
    "- Exploring document boundaries and offsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2baf13",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our environment, similar to the first tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d2bfe51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-01 12:19:08,855] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/NS/venvs/work/afkhan/neox_updated_env/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @autocast_custom_fwd\n",
      "/NS/venvs/work/afkhan/neox_updated_env/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @autocast_custom_bwd\n",
      "/NS/venvs/work/afkhan/neox_updated_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/NS/venvs/work/afkhan/neox_updated_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer: EleutherAI/gpt-neox-20b\n"
     ]
    }
   ],
   "source": [
    "# Fix paths for imports\n",
    "import sys\n",
    "sys.path.insert(0, \"/NS/llm-pretraining/work/afkhan/tokensmith\")\n",
    "sys.path.insert(0, \"/NS/llm-pretraining/work/afkhan/USC_Colab/gpt-neox\")\n",
    "\n",
    "# Import required libraries\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from tokensmith.manager import DatasetManager\n",
    "\n",
    "# Load tokenizer\n",
    "TOKENIZER_NAME_OR_PATH = \"EleutherAI/gpt-neox-20b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME_OR_PATH, add_eos_token=True)\n",
    "print(f\"Loaded tokenizer: {TOKENIZER_NAME_OR_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19314ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    warming up index mmap file...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "Dataset manager setup complete!\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "Dataset manager setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Initialize DatasetManager and setup for inspection\n",
    "dataset_manager = DatasetManager()\n",
    "\n",
    "# Setup the dataset for inspection (same as tutorial 1)\n",
    "dataset_manager.setup_edit_inspect_sample_export(\n",
    "    dataset_prefix='../../artifacts/data_tokenized_text_document',\n",
    "    batch_info_save_prefix='../../artifacts/batch_info',\n",
    "    train_iters=100,\n",
    "    train_batch_size=16,\n",
    "    train_seq_len=2048,\n",
    "    seed=42,\n",
    "    splits_string='990,5,5',\n",
    "    packing_impl='packed',\n",
    "    allow_chopped=True,\n",
    ")\n",
    "print(\"Dataset manager setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb87ad04",
   "metadata": {},
   "source": [
    "## Basic Sample Inspection\n",
    "\n",
    "Let's start by inspecting individual samples. We'll look at sample ID 0 and understand what information we can extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99965f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0 (tokenized):\n",
      "Type: <class 'list'>\n",
      "Number of segments: 12\n",
      "First segment shape: (70,)\n",
      "First 10 tokens: [ 2181  4592    15 32817   434  1652  4929  2210  3515   285]\n"
     ]
    }
   ],
   "source": [
    "# Inspect the first sample (ID: 0) - returns tokenized data\n",
    "sample_0 = dataset_manager.inspect.inspect_sample_by_id(sample_id=0)\n",
    "\n",
    "print(\"Sample 0 (tokenized):\")\n",
    "print(f\"Type: {type(sample_0)}\")\n",
    "print(f\"Number of segments: {len(sample_0)}\")\n",
    "print(f\"First segment shape: {sample_0[0].shape}\")\n",
    "print(f\"First 10 tokens: {sample_0[0][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf236b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0 (detokenized text):\n",
      "Type: <class 'str'>\n",
      "Length: 8990 characters\n",
      "\n",
      "First 200 characters:\n",
      " thing happened. Lily's little brother came running and accidentally stepped on the diamond. Oh no! The diamond was destroyed! Lily was very sad, but her mommy and daddy told her that it's okay becaus\n",
      "\n",
      "==================================================\n",
      "Last 200 characters:\n",
      "listening to the birds tweet and trying to catch a glimpse of a rabbit. Soon enough, they found the perfect spot to escape and set up a secret camp. They explored the forest, gathered flowers and made\n"
     ]
    }
   ],
   "source": [
    "# Now let's see the same sample but detokenized (human-readable text)\n",
    "sample_0_text = dataset_manager.inspect.inspect_sample_by_id(\n",
    "    sample_id=0, \n",
    "    return_detokenized=True, \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(\"Sample 0 (detokenized text):\")\n",
    "print(f\"Type: {type(sample_0_text)}\")\n",
    "print(f\"Length: {len(sample_0_text)} characters\")\n",
    "print(\"\\nFirst 200 characters:\")\n",
    "print(sample_0_text[:200])\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Last 200 characters:\")\n",
    "print(sample_0_text[-200:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5c1621",
   "metadata": {},
   "source": [
    "## Understanding Document Details\n",
    "\n",
    "TokenSmith can also provide metadata about each sample, including document boundaries and offsets. This is useful for understanding how your data was packed and segmented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3e963ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0 - Document Details:\n",
      "Document details type: <class 'dict'>\n",
      "Document metadata:\n",
      "  doc_index_f: 11212\n",
      "  doc_index_l: 11223\n",
      "  offset_f: 67\n",
      "  offset_l: 154\n"
     ]
    }
   ],
   "source": [
    "# Get sample with document details\n",
    "sample_0_with_details = dataset_manager.inspect.inspect_sample_by_id(\n",
    "    sample_id=0, \n",
    "    return_doc_details=True\n",
    ")\n",
    "\n",
    "tokens, doc_details = sample_0_with_details\n",
    "\n",
    "print(\"Sample 0 - Document Details:\")\n",
    "print(f\"Document details type: {type(doc_details)}\")\n",
    "print(\"Document metadata:\")\n",
    "for key, value in doc_details.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5317c502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0 - Text with Document Details:\n",
      "Text length: 8990 characters\n",
      "\n",
      "Document metadata:\n",
      "  doc_index_f: 11212\n",
      "  doc_index_l: 11223\n",
      "  offset_f: 67\n",
      "  offset_l: 154\n",
      "\n",
      "First 100 characters:\n",
      " thing happened. Lily's little brother came running and accidentally stepped on the diamond. Oh no! \n"
     ]
    }
   ],
   "source": [
    "# Get both detokenized text AND document details\n",
    "sample_0_text_with_details = dataset_manager.inspect.inspect_sample_by_id(\n",
    "    sample_id=0, \n",
    "    return_detokenized=True, \n",
    "    return_doc_details=True, \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "text, doc_details = sample_0_text_with_details\n",
    "\n",
    "print(\"Sample 0 - Text with Document Details:\")\n",
    "print(f\"Text length: {len(text)} characters\")\n",
    "print(\"\\nDocument metadata:\")\n",
    "for key, value in doc_details.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "    \n",
    "print(f\"\\nFirst 100 characters:\\n{text[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393fddec",
   "metadata": {},
   "source": [
    "## Inspecting Multiple Samples\n",
    "\n",
    "Let's look at several samples to understand the variation in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1e37d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting multiple samples:\n",
      "============================================================\n",
      "\n",
      "Sample ID: 0\n",
      "Text length: 8990 characters\n",
      "Doc index range: 11212 to 11223\n",
      "Offset range: 67 to 154\n",
      "Preview:  thing happened. Lily's little brother came running and accidentally stepped on the diamond. Oh no! ...\n",
      "----------------------------------------\n",
      "\n",
      "Sample ID: 1\n",
      "Text length: 8388 characters\n",
      "Doc index range: 15126 to 15133\n",
      "Offset range: 129 to 151\n",
      "Preview: . She had gone to the office for a minute. Lily had an idea. \"Let's steal some crayons,\" she whisper...\n",
      "----------------------------------------\n",
      "\n",
      "Sample ID: 5\n",
      "Text length: 8404 characters\n",
      "Doc index range: 5991 to 5998\n",
      "Offset range: 226 to 134\n",
      "Preview: Let's look at the pictures. They might tell us something.\" Lila and Ben look at the pictures on the ...\n",
      "----------------------------------------\n",
      "\n",
      "Sample ID: 10\n",
      "Text length: 8530 characters\n",
      "Doc index range: 7983 to 7994\n",
      "Offset range: 16 to 4\n",
      "Preview:  day, they find a big club on the grass. It is brown and heavy. \"Look, a club!\" Lily says. \"Let's pl...\n",
      "----------------------------------------\n",
      "\n",
      "Sample ID: 50\n",
      "Text length: 8495 characters\n",
      "Doc index range: 14417 to 14425\n",
      "Offset range: 215 to 176\n",
      "Preview:  Ben's car fell on the ground and broke. The wheel came off and the paint scratched. \"Uh oh!\" Lily s...\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Inspect multiple individual samples\n",
    "sample_ids_to_check = [0, 1, 5, 10, 50]\n",
    "\n",
    "print(\"Inspecting multiple samples:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for sample_id in sample_ids_to_check:\n",
    "    # Get detokenized text with document details\n",
    "    text, doc_details = dataset_manager.inspect.inspect_sample_by_id(\n",
    "        sample_id=sample_id,\n",
    "        return_detokenized=True,\n",
    "        return_doc_details=True,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nSample ID: {sample_id}\")\n",
    "    print(f\"Text length: {len(text)} characters\")\n",
    "    print(f\"Doc index range: {doc_details['doc_index_f']} to {doc_details['doc_index_l']}\")\n",
    "    print(f\"Offset range: {doc_details['offset_f']} to {doc_details['offset_l']}\")\n",
    "    print(f\"Preview: {text[:100]}...\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79193b01",
   "metadata": {},
   "source": [
    "## Batch Inspection\n",
    "\n",
    "TokenSmith also allows you to inspect entire batches at once, which is useful for understanding how your training batches will look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57fe9423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 inspection:\n",
      "Batch type: <class 'list'>\n",
      "Number of samples in batch: 4\n",
      "\n",
      "--- Sample 0 in batch ---\n",
      "Length: 8990 characters\n",
      "Preview:  thing happened. Lily's little brother came running and accidentally stepped on ...\n",
      "\n",
      "--- Sample 1 in batch ---\n",
      "Length: 8388 characters\n",
      "Preview: . She had gone to the office for a minute. Lily had an idea. \"Let's steal some c...\n",
      "\n",
      "--- Sample 2 in batch ---\n",
      "Length: 8789 characters\n",
      "Preview:  agreed to marry him. They had a wonderful wedding and were very happy together....\n",
      "\n",
      "--- Sample 3 in batch ---\n",
      "Length: 8700 characters\n",
      "Preview:  sleep, Maggie's mommy saw something very rare and wet. It was raining outside a...\n"
     ]
    }
   ],
   "source": [
    "# Inspect batch 0 (first batch of samples)\n",
    "batch_0 = dataset_manager.inspect.inspect_sample_by_batch(\n",
    "    batch_id=0,\n",
    "    batch_size=4,  # Let's use a smaller batch size for easier inspection\n",
    "    return_detokenized=True,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(f\"Batch 0 inspection:\")\n",
    "print(f\"Batch type: {type(batch_0)}\")\n",
    "print(f\"Number of samples in batch: {len(batch_0)}\")\n",
    "\n",
    "for i, sample_text in enumerate(batch_0):\n",
    "    print(f\"\\n--- Sample {i} in batch ---\")\n",
    "    print(f\"Length: {len(sample_text)} characters\")\n",
    "    print(f\"Preview: {sample_text[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfdf63d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 with document details:\n",
      "Batch size: 4\n",
      "\n",
      "--- Sample 0 in batch ---\n",
      "Text length: 8990 characters\n",
      "Document range: docs 11212-11223\n",
      "Offset range: 67-154\n",
      "Preview:  thing happened. Lily's little brother came running and acci...\n",
      "\n",
      "--- Sample 1 in batch ---\n",
      "Text length: 8388 characters\n",
      "Document range: docs 15126-15133\n",
      "Offset range: 129-151\n",
      "Preview: . She had gone to the office for a minute. Lily had an idea....\n",
      "\n",
      "--- Sample 2 in batch ---\n",
      "Text length: 8789 characters\n",
      "Document range: docs 9100-9111\n",
      "Offset range: 61-116\n",
      "Preview:  agreed to marry him. They had a wonderful wedding and were ...\n",
      "\n",
      "--- Sample 3 in batch ---\n",
      "Text length: 8700 characters\n",
      "Document range: docs 5168-5178\n",
      "Offset range: 110-202\n",
      "Preview:  sleep, Maggie's mommy saw something very rare and wet. It w...\n"
     ]
    }
   ],
   "source": [
    "# Inspect batch with document details\n",
    "batch_0_with_details = dataset_manager.inspect.inspect_sample_by_batch(\n",
    "    batch_id=0,\n",
    "    batch_size=4,\n",
    "    return_detokenized=True,\n",
    "    return_doc_details=True,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(\"Batch 0 with document details:\")\n",
    "print(f\"Batch size: {len(batch_0_with_details)}\")\n",
    "\n",
    "for i, (sample_text, doc_details) in enumerate(batch_0_with_details):\n",
    "    print(f\"\\n--- Sample {i} in batch ---\")\n",
    "    print(f\"Text length: {len(sample_text)} characters\")\n",
    "    print(f\"Document range: docs {doc_details['doc_index_f']}-{doc_details['doc_index_l']}\")\n",
    "    print(f\"Offset range: {doc_details['offset_f']}-{doc_details['offset_l']}\")\n",
    "    print(f\"Preview: {sample_text[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe8362d",
   "metadata": {},
   "source": [
    "## Understanding Tokenization Patterns\n",
    "\n",
    "Let's examine how different types of content get tokenized to better understand our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "337b8ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of Sample 5:\n",
      "Number of token segments: 8\n",
      "Total tokens: 2049\n",
      "Total characters: 8404\n",
      "Average tokens per character: 0.244\n",
      "\n",
      "Token distribution across segments:\n",
      "  Segment 0: 495 tokens\n",
      "  Segment 1: 173 tokens\n",
      "  Segment 2: 179 tokens\n",
      "  Segment 3: 228 tokens\n",
      "  Segment 4: 256 tokens\n",
      "  Segment 5: 171 tokens\n",
      "  Segment 6: 412 tokens\n",
      "  Segment 7: 135 tokens\n"
     ]
    }
   ],
   "source": [
    "# Compare tokenized vs detokenized for analysis\n",
    "sample_id = 5\n",
    "\n",
    "# Get tokenized version (raw tokens)\n",
    "tokens = dataset_manager.inspect.inspect_sample_by_id(sample_id=sample_id)\n",
    "\n",
    "# Get detokenized version (text)\n",
    "text = dataset_manager.inspect.inspect_sample_by_id(\n",
    "    sample_id=sample_id, \n",
    "    return_detokenized=True, \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(f\"Analysis of Sample {sample_id}:\")\n",
    "print(f\"Number of token segments: {len(tokens)}\")\n",
    "\n",
    "total_tokens = sum(len(segment) for segment in tokens)\n",
    "print(f\"Total tokens: {total_tokens}\")\n",
    "print(f\"Total characters: {len(text)}\")\n",
    "print(f\"Average tokens per character: {total_tokens/len(text):.3f}\")\n",
    "\n",
    "print(f\"\\nToken distribution across segments:\")\n",
    "for i, segment in enumerate(tokens):\n",
    "    print(f\"  Segment {i}: {len(segment)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91daebb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID to text mapping (first 20 tokens):\n",
      "Token ID | Decoded Text\n",
      "------------------------------\n",
      "    1466 | 'Let'\n",
      "     434 | \"'s\"\n",
      "    1007 | ' look'\n",
      "     387 | ' at'\n",
      "     253 | ' the'\n",
      "    7968 | ' pictures'\n",
      "      15 | '.'\n",
      "    1583 | ' They'\n",
      "    1537 | ' might'\n",
      "    2028 | ' tell'\n",
      "     441 | ' us'\n",
      "    1633 | ' something'\n",
      "     449 | '.\"'\n",
      "     418 | ' L'\n",
      "    8807 | 'ila'\n",
      "     285 | ' and'\n",
      "    6029 | ' Ben'\n",
      "    1007 | ' look'\n",
      "     387 | ' at'\n",
      "     253 | ' the'\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the actual token IDs and their decoded values\n",
    "sample_tokens = tokens[0][:20]  # First 20 tokens from first segment\n",
    "decoded_tokens = [tokenizer.decode([token_id]) for token_id in sample_tokens]\n",
    "\n",
    "print(\"Token ID to text mapping (first 20 tokens):\")\n",
    "print(\"Token ID | Decoded Text\")\n",
    "print(\"-\" * 30)\n",
    "for token_id, decoded_text in zip(sample_tokens, decoded_tokens):\n",
    "    # Clean up the decoded text for display\n",
    "    display_text = repr(decoded_text)\n",
    "    print(f\"{token_id:8d} | {display_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d58e27e",
   "metadata": {},
   "source": [
    "## Advanced Inspection: Cross-Document Boundaries\n",
    "\n",
    "Let's examine samples that might span multiple documents to understand how packing works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0aeb52d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 samples spanning multiple documents:\n",
      "\n",
      "Sample 0:\n",
      "  Spans documents 11212 to 11223\n",
      "  Offset range: 67 to 154\n",
      "  Text length: 8990 characters\n",
      "  Preview:  thing happened. Lily's little brother came running and accidentally stepped on the diamond. Oh no! ...\n",
      "\n",
      "Sample 1:\n",
      "  Spans documents 15126 to 15133\n",
      "  Offset range: 129 to 151\n",
      "  Text length: 8388 characters\n",
      "  Preview: . She had gone to the office for a minute. Lily had an idea. \"Let's steal some crayons,\" she whisper...\n",
      "\n",
      "Sample 2:\n",
      "  Spans documents 9100 to 9111\n",
      "  Offset range: 61 to 116\n",
      "  Text length: 8789 characters\n",
      "  Preview:  agreed to marry him. They had a wonderful wedding and were very happy together. They lived happily ...\n"
     ]
    }
   ],
   "source": [
    "# Find samples that span multiple documents\n",
    "samples_with_multi_docs = []\n",
    "\n",
    "for sample_id in range(20):  # Check first 20 samples\n",
    "    _, doc_details = dataset_manager.inspect.inspect_sample_by_id(\n",
    "        sample_id=sample_id,\n",
    "        return_doc_details=True\n",
    "    )\n",
    "    \n",
    "    # Check if this sample spans multiple documents\n",
    "    if doc_details['doc_index_f'] != doc_details['doc_index_l']:\n",
    "        samples_with_multi_docs.append((sample_id, doc_details))\n",
    "\n",
    "print(f\"Found {len(samples_with_multi_docs)} samples spanning multiple documents:\")\n",
    "\n",
    "for sample_id, doc_details in samples_with_multi_docs[:3]:  # Show first 3\n",
    "    text = dataset_manager.inspect.inspect_sample_by_id(\n",
    "        sample_id=sample_id,\n",
    "        return_detokenized=True,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nSample {sample_id}:\")\n",
    "    print(f\"  Spans documents {doc_details['doc_index_f']} to {doc_details['doc_index_l']}\")\n",
    "    print(f\"  Offset range: {doc_details['offset_f']} to {doc_details['offset_l']}\")\n",
    "    print(f\"  Text length: {len(text)} characters\")\n",
    "    print(f\"  Preview: {text[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4cb060",
   "metadata": {},
   "source": [
    "## Practical Tips for Dataset Inspection\n",
    "\n",
    "Here are some useful patterns for inspecting your dataset during development and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f191f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0 Summary:\n",
      "  total_tokens: 2049\n",
      "  total_chars: 8990\n",
      "  num_segments: 12\n",
      "  doc_range: 11212-11223\n",
      "  offset_range: 67-154\n",
      "  spans_multiple_docs: True\n",
      "  preview:  thing happened. Lily's little brother came runnin...\n",
      "\n",
      "Sample 10 Summary:\n",
      "  total_tokens: 2049\n",
      "  total_chars: 8530\n",
      "  num_segments: 12\n",
      "  doc_range: 7983-7994\n",
      "  offset_range: 16-4\n",
      "  spans_multiple_docs: True\n",
      "  preview:  day, they find a big club on the grass. It is bro...\n",
      "\n",
      "Sample 25 Summary:\n",
      "  total_tokens: 2049\n",
      "  total_chars: 8254\n",
      "  num_segments: 8\n",
      "  doc_range: 11194-11201\n",
      "  offset_range: 62-336\n",
      "  spans_multiple_docs: True\n",
      "  preview:  that the sun made droplets scatter off of their b...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def quick_sample_summary(dataset_manager, sample_id, tokenizer):\n",
    "    \"\"\"Helper function to get a quick summary of any sample\"\"\"\n",
    "    \n",
    "    # Get both tokenized and text versions with details\n",
    "    tokens = dataset_manager.inspect.inspect_sample_by_id(sample_id=sample_id)\n",
    "    text, doc_details = dataset_manager.inspect.inspect_sample_by_id(\n",
    "        sample_id=sample_id,\n",
    "        return_detokenized=True,\n",
    "        return_doc_details=True,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    total_tokens = sum(len(segment) for segment in tokens)\n",
    "    \n",
    "    summary = {\n",
    "        'sample_id': sample_id,\n",
    "        'total_tokens': total_tokens,\n",
    "        'total_chars': len(text),\n",
    "        'num_segments': len(tokens),\n",
    "        'doc_range': f\"{doc_details['doc_index_f']}-{doc_details['doc_index_l']}\",\n",
    "        'offset_range': f\"{doc_details['offset_f']}-{doc_details['offset_l']}\",\n",
    "        'spans_multiple_docs': doc_details['doc_index_f'] != doc_details['doc_index_l'],\n",
    "        'preview': text[:50] + \"...\" if len(text) > 50 else text\n",
    "    }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Test our helper function\n",
    "for sample_id in [0, 10, 25]:\n",
    "    summary = quick_sample_summary(dataset_manager, sample_id, tokenizer)\n",
    "    print(f\"Sample {sample_id} Summary:\")\n",
    "    for key, value in summary.items():\n",
    "        if key != 'sample_id':\n",
    "            print(f\"  {key}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "338a4484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 Statistics:\n",
      "  Samples: 4\n",
      "  Total characters: 34,867\n",
      "  Average length: 8716.8\n",
      "  Length range: 8388-8990\n",
      "  Multi-document samples: 4\n",
      "\n",
      "Batch 1 Statistics:\n",
      "  Samples: 4\n",
      "  Total characters: 34,649\n",
      "  Average length: 8662.2\n",
      "  Length range: 8404-8976\n",
      "  Multi-document samples: 4\n",
      "\n",
      "Batch 2 Statistics:\n",
      "  Samples: 4\n",
      "  Total characters: 34,621\n",
      "  Average length: 8655.2\n",
      "  Length range: 8530-8813\n",
      "  Multi-document samples: 4\n",
      "\n",
      "Batch 2 Statistics:\n",
      "  Samples: 4\n",
      "  Total characters: 34,621\n",
      "  Average length: 8655.2\n",
      "  Length range: 8530-8813\n",
      "  Multi-document samples: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def batch_statistics(dataset_manager, batch_id, batch_size, tokenizer):\n",
    "    \"\"\"Get statistics for an entire batch\"\"\"\n",
    "    \n",
    "    batch_data = dataset_manager.inspect.inspect_sample_by_batch(\n",
    "        batch_id=batch_id,\n",
    "        batch_size=batch_size,\n",
    "        return_detokenized=True,\n",
    "        return_doc_details=True,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    stats = {\n",
    "        'batch_id': batch_id,\n",
    "        'batch_size': len(batch_data),\n",
    "        'text_lengths': [],\n",
    "        'multi_doc_samples': 0,\n",
    "        'total_chars': 0\n",
    "    }\n",
    "    \n",
    "    for text, doc_details in batch_data:\n",
    "        stats['text_lengths'].append(len(text))\n",
    "        stats['total_chars'] += len(text)\n",
    "        if doc_details['doc_index_f'] != doc_details['doc_index_l']:\n",
    "            stats['multi_doc_samples'] += 1\n",
    "    \n",
    "    stats['avg_length'] = stats['total_chars'] / stats['batch_size']\n",
    "    stats['min_length'] = min(stats['text_lengths'])\n",
    "    stats['max_length'] = max(stats['text_lengths'])\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Get statistics for first few batches\n",
    "for batch_id in range(3):\n",
    "    stats = batch_statistics(dataset_manager, batch_id, 4, tokenizer)\n",
    "    print(f\"Batch {batch_id} Statistics:\")\n",
    "    print(f\"  Samples: {stats['batch_size']}\")\n",
    "    print(f\"  Total characters: {stats['total_chars']:,}\")\n",
    "    print(f\"  Average length: {stats['avg_length']:.1f}\")\n",
    "    print(f\"  Length range: {stats['min_length']}-{stats['max_length']}\")\n",
    "    print(f\"  Multi-document samples: {stats['multi_doc_samples']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc6f845",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've successfully learned how to inspect your tokenized dataset using TokenSmith. Here's what we covered:\n",
    "\n",
    "### Key Concepts Learned:\n",
    "1. **Individual Sample Inspection**: How to retrieve and examine single samples by ID\n",
    "2. **Batch Inspection**: How to inspect multiple samples as batches\n",
    "3. **Document Details**: Understanding metadata about document boundaries and offsets\n",
    "4. **Tokenized vs Detokenized**: Working with both token arrays and human-readable text\n",
    "5. **Cross-Document Analysis**: Identifying samples that span multiple source documents\n",
    "6. **Practical Utilities**: Creating helper functions for routine inspection tasks\n",
    "\n",
    "### Key Methods Used:\n",
    "- `dataset_manager.inspect.inspect_sample_by_id()` - Inspect individual samples\n",
    "- `dataset_manager.inspect.inspect_sample_by_batch()` - Inspect batches of samples\n",
    "- Parameters: `return_doc_details`, `return_detokenized`, `tokenizer`\n",
    "\n",
    "### Next Steps:\n",
    "- Tutorial 3: Learn about different sampling methods and policies\n",
    "- Tutorial 4: Explore search functionality across your dataset\n",
    "- Tutorial 5: Understand editing and injection capabilities\n",
    "\n",
    "### Pro Tips:\n",
    "- Always use `return_doc_details=True` when debugging data packing issues\n",
    "- Create helper functions for routine inspection tasks\n",
    "- Use batch inspection to understand training data patterns\n",
    "- Compare tokenized and detokenized versions to verify data integrity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neox_updated_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
