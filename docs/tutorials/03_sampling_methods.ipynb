{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99203e91",
   "metadata": {},
   "source": [
    "# Dataset Sampling Methods & Policies Tutorial\n",
    "\n",
    "This tutorial demonstrates how to use TokenSmith's flexible sampling functionality to retrieve data samples and batches using various strategies. We'll explore different sampling methods, custom policies, and advanced techniques for data selection.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Complete tutorials 1 and 2 (basic setup and inspection)\n",
    "- Have a tokenized dataset ready with batch info generated\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to sample data by specific indices\n",
    "- How to sample batches by IDs\n",
    "- Creating and using custom sampling policies\n",
    "- Policy-based sampling for individual samples and batches\n",
    "- Advanced sampling strategies for research and analysis\n",
    "- Performance considerations for different sampling methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d879c58a",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by setting up our environment and dataset manager, building on the previous tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bf93803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-01 12:32:22,370] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/NS/venvs/work/afkhan/neox_updated_env/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @autocast_custom_fwd\n",
      "/NS/venvs/work/afkhan/neox_updated_env/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @autocast_custom_bwd\n",
      "/NS/venvs/work/afkhan/neox_updated_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer: EleutherAI/gpt-neox-20b\n"
     ]
    }
   ],
   "source": [
    "# Fix paths for imports\n",
    "import sys\n",
    "sys.path.insert(0, \"/NS/llm-pretraining/work/afkhan/tokensmith\")\n",
    "sys.path.insert(0, \"/NS/llm-pretraining/work/afkhan/USC_Colab/gpt-neox\")\n",
    "\n",
    "# Import required libraries\n",
    "import numpy as np\n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "from tokensmith.manager import DatasetManager\n",
    "\n",
    "# Load tokenizer\n",
    "TOKENIZER_NAME_OR_PATH = \"EleutherAI/gpt-neox-20b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME_OR_PATH, add_eos_token=True)\n",
    "print(f\"Loaded tokenizer: {TOKENIZER_NAME_OR_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d77fb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    warming up index mmap file...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "Dataset manager setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Initialize DatasetManager\n",
    "dataset_manager = DatasetManager()\n",
    "\n",
    "# Setup the dataset for sampling\n",
    "dataset_manager.setup_edit_inspect_sample_export(\n",
    "    dataset_prefix='../../artifacts/data_tokenized_text_document',\n",
    "    batch_info_save_prefix='../../artifacts/batch_info',\n",
    "    train_iters=100,\n",
    "    train_batch_size=16,\n",
    "    train_seq_len=2048,\n",
    "    seed=42,\n",
    "    splits_string='990,5,5',\n",
    "    packing_impl='packed',\n",
    "    allow_chopped=True,\n",
    ")\n",
    "print(\"Dataset manager setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca26130a",
   "metadata": {},
   "source": [
    "## Basic Sampling by Indices\n",
    "\n",
    "The most straightforward way to sample data is by specifying exact sample indices. This is useful when you know exactly which samples you want to examine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c5ae0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized samples:\n",
      "Sample 0: 12 segments, 2049 total tokens\n",
      "  First segment shape: (70,)\n",
      "  First 10 tokens: [ 2181  4592    15 32817   434  1652  4929  2210  3515   285]\n",
      "\n",
      "Sample 5: 8 segments, 2049 total tokens\n",
      "  First segment shape: (495,)\n",
      "  First 10 tokens: [1466  434 1007  387  253 7968   15 1583 1537 2028]\n",
      "\n",
      "Sample 10: 12 segments, 2049 total tokens\n",
      "  First segment shape: (291,)\n",
      "  First 10 tokens: [ 1388    13   597  1089   247  1943  5453   327   253 10583]\n",
      "\n",
      "Sample 25: 8 segments, 2049 total tokens\n",
      "  First segment shape: (55,)\n",
      "  First 10 tokens: [  326   253  5101  1160 30244 24493   745   273   616 22513]\n",
      "\n",
      "Sample 50: 9 segments, 2049 total tokens\n",
      "  First segment shape: (261,)\n",
      "  First 10 tokens: [6029  434 1113 6497  327  253 3216  285 9377   15]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample specific indices\n",
    "sample_indices = [0, 5, 10, 25, 50]\n",
    "\n",
    "# Get samples as tokenized arrays\n",
    "tokenized_samples = dataset_manager.sample.get_samples_by_indices(\n",
    "    indices=sample_indices\n",
    ")\n",
    "\n",
    "print(\"Tokenized samples:\")\n",
    "for i, (idx, sample) in enumerate(zip(sample_indices, tokenized_samples)):\n",
    "    total_tokens = sum(len(segment) for segment in sample)\n",
    "    print(f\"Sample {idx}: {len(sample)} segments, {total_tokens} total tokens\")\n",
    "    print(f\"  First segment shape: {sample[0].shape}\")\n",
    "    print(f\"  First 10 tokens: {sample[0][:10]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8436148d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detokenized samples:\n",
      "Sample 0 (length: 8990 chars):\n",
      "  Preview:  thing happened. Lily's little brother came running and accidentally stepped on the diamond. Oh no! The diamond was destroyed! Lily was very sad, but ...\n",
      "  Ending: ...hey explored the forest, gathered flowers and made\n",
      "\n",
      "Sample 5 (length: 8404 chars):\n",
      "  Preview: Let's look at the pictures. They might tell us something.\" Lila and Ben look at the pictures on the map. They see a sun, a cloud, a star, a fish, and ...\n",
      "  Ending: ...ets on!\" They both put on their bracelets and rode\n",
      "\n",
      "Sample 10 (length: 8530 chars):\n",
      "  Preview:  day, they find a big club on the grass. It is brown and heavy. \"Look, a club!\" Lily says. \"Let's play with it!\" \"OK!\" Ben says. \"What can we do with ...\n",
      "  Ending: ...t was scary but fun.<|endoftext|>Once upon a time,\n",
      "\n",
      "Sample 25 (length: 8254 chars):\n",
      "  Preview:  that the sun made droplets scatter off of their backs! They felt so refreshed in the warm, wet water. After a while, the frogs decided it was time to...\n",
      "  Ending: ... to Ben. \"I'm sorry, Ben,\" she said. \"I was wrong.\n",
      "\n",
      "Sample 50 (length: 8495 chars):\n",
      "  Preview:  Ben's car fell on the ground and broke. The wheel came off and the paint scratched. \"Uh oh!\" Lily said, looking at Ben's car. \"I'm sorry, Ben. I did ...\n",
      "  Ending: ...r and closed her eyes. She tried to think of happy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Same samples but detokenized (human-readable text)\n",
    "text_samples = dataset_manager.sample.get_samples_by_indices(\n",
    "    indices=sample_indices,\n",
    "    return_detokenized=True,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(\"Detokenized samples:\")\n",
    "for idx, text in zip(sample_indices, text_samples):\n",
    "    print(f\"Sample {idx} (length: {len(text)} chars):\")\n",
    "    print(f\"  Preview: {text[:150]}...\")\n",
    "    print(f\"  Ending: ...{text[-50:]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d6b632a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples with document details:\n",
      "Sample 0:\n",
      "  Text length: 8990 characters\n",
      "  Document range: docs 11212 to 11223\n",
      "  Offset range: 67 to 154\n",
      "  Spans multiple docs: True\n",
      "  Preview:  thing happened. Lily's little brother came running and accidentally stepped on the diamond. Oh no! ...\n",
      "\n",
      "Sample 5:\n",
      "  Text length: 8404 characters\n",
      "  Document range: docs 5991 to 5998\n",
      "  Offset range: 226 to 134\n",
      "  Spans multiple docs: True\n",
      "  Preview: Let's look at the pictures. They might tell us something.\" Lila and Ben look at the pictures on the ...\n",
      "\n",
      "Sample 10:\n",
      "  Text length: 8530 characters\n",
      "  Document range: docs 7983 to 7994\n",
      "  Offset range: 16 to 4\n",
      "  Spans multiple docs: True\n",
      "  Preview:  day, they find a big club on the grass. It is brown and heavy. \"Look, a club!\" Lily says. \"Let's pl...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get samples with document details\n",
    "samples_with_details = dataset_manager.sample.get_samples_by_indices(\n",
    "    indices=sample_indices[:3],  # Just first 3 for brevity\n",
    "    return_detokenized=True,\n",
    "    return_doc_details=True,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(\"Samples with document details:\")\n",
    "for idx, (text, doc_details) in zip(sample_indices[:3], samples_with_details):\n",
    "    print(f\"Sample {idx}:\")\n",
    "    print(f\"  Text length: {len(text)} characters\")\n",
    "    print(f\"  Document range: docs {doc_details['doc_index_f']} to {doc_details['doc_index_l']}\")\n",
    "    print(f\"  Offset range: {doc_details['offset_f']} to {doc_details['offset_l']}\")\n",
    "    print(f\"  Spans multiple docs: {doc_details['doc_index_f'] != doc_details['doc_index_l']}\")\n",
    "    print(f\"  Preview: {text[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9a27b8",
   "metadata": {},
   "source": [
    "## Batch Sampling by IDs\n",
    "\n",
    "When working with training data, you often want to examine entire batches as they would appear during training. TokenSmith allows you to sample specific batches by their IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a51dc8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 3 batches:\n",
      "\n",
      "Batch 0 (size: 4):\n",
      "  Sample 0 (global ID 0): 8990 chars\n",
      "    Preview:  thing happened. Lily's little brother came running and accidentally stepped on ...\n",
      "  Sample 1 (global ID 1): 8388 chars\n",
      "    Preview: . She had gone to the office for a minute. Lily had an idea. \"Let's steal some c...\n",
      "  Sample 2 (global ID 2): 8789 chars\n",
      "    Preview:  agreed to marry him. They had a wonderful wedding and were very happy together....\n",
      "  Sample 3 (global ID 3): 8700 chars\n",
      "    Preview:  sleep, Maggie's mommy saw something very rare and wet. It was raining outside a...\n",
      "\n",
      "Batch 2 (size: 4):\n",
      "  Sample 0 (global ID 8): 8547 chars\n",
      "    Preview:  park. They see the slide and the swing. They wish they had been nice. They wish...\n",
      "  Sample 1 (global ID 9): 8731 chars\n",
      "    Preview:  run around the faucet, letting the water spray all over him. His mom, seeing th...\n",
      "  Sample 2 (global ID 10): 8530 chars\n",
      "    Preview:  day, they find a big club on the grass. It is brown and heavy. \"Look, a club!\" ...\n",
      "  Sample 3 (global ID 11): 8813 chars\n",
      "    Preview:  and dad said it seemed too dangerous, so they said no. The boy didn't listen an...\n",
      "\n",
      "Batch 5 (size: 4):\n",
      "  Sample 0 (global ID 20): 8650 chars\n",
      "    Preview:  saying thank you and sorry. They did not know that they had missed a chance to ...\n",
      "  Sample 1 (global ID 21): 8398 chars\n",
      "    Preview:  it and take care of it. It will be your new friend.\" Tim and Lily hug their par...\n",
      "  Sample 2 (global ID 22): 8857 chars\n",
      "    Preview:  Suddenly, a big, brown pony appeared in front of her! Lily was so happy and hug...\n",
      "  Sample 3 (global ID 23): 8998 chars\n",
      "    Preview:  also about being kind and working hard. From that day on, Timmy worked hard and...\n"
     ]
    }
   ],
   "source": [
    "# Sample specific batches\n",
    "batch_ids = [0, 2, 5]\n",
    "batch_size = 4  # Small batch size for easier examination\n",
    "\n",
    "batches = dataset_manager.sample.get_batches_by_ids(\n",
    "    batch_ids=batch_ids,\n",
    "    batch_size=batch_size,\n",
    "    return_detokenized=True,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(f\"Sampled {len(batches)} batches:\")\n",
    "for batch_idx, batch in enumerate(batches):\n",
    "    batch_id = batch_ids[batch_idx]\n",
    "    print(f\"\\nBatch {batch_id} (size: {len(batch)}):\")\n",
    "    for sample_idx, sample in enumerate(batch):\n",
    "        global_sample_id = batch_id * batch_size + sample_idx\n",
    "        print(f\"  Sample {sample_idx} (global ID {global_sample_id}): {len(sample)} chars\")\n",
    "        print(f\"    Preview: {sample[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52b5baa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches with document details:\n",
      "\n",
      "Batch 1:\n",
      "  Sample 0 (global ID 3):\n",
      "    Length: 8700 chars\n",
      "    Doc range: 5168-5178\n",
      "    Multi-doc: True\n",
      "    Preview:  sleep, Maggie's mommy saw something very rare and wet. It w...\n",
      "  Sample 1 (global ID 4):\n",
      "    Length: 8976 chars\n",
      "    Doc range: 1942-1954\n",
      "    Multi-doc: True\n",
      "    Preview:  Benny learned that not all big, wild animals are scary. Som...\n",
      "  Sample 2 (global ID 5):\n",
      "    Length: 8404 chars\n",
      "    Doc range: 5991-5998\n",
      "    Multi-doc: True\n",
      "    Preview: Let's look at the pictures. They might tell us something.\" L...\n",
      "\n",
      "Batch 3:\n",
      "  Sample 0 (global ID 9):\n",
      "    Length: 8731 chars\n",
      "    Doc range: 6888-6899\n",
      "    Multi-doc: True\n",
      "    Preview:  run around the faucet, letting the water spray all over him...\n",
      "  Sample 1 (global ID 10):\n",
      "    Length: 8530 chars\n",
      "    Doc range: 7983-7994\n",
      "    Multi-doc: True\n",
      "    Preview:  day, they find a big club on the grass. It is brown and hea...\n",
      "  Sample 2 (global ID 11):\n",
      "    Length: 8813 chars\n",
      "    Doc range: 13066-13077\n",
      "    Multi-doc: True\n",
      "    Preview:  and dad said it seemed too dangerous, so they said no. The ...\n"
     ]
    }
   ],
   "source": [
    "# Get batches with document details\n",
    "batches_with_details = dataset_manager.sample.get_batches_by_ids(\n",
    "    batch_ids=[1, 3],  # Just 2 batches for detailed examination\n",
    "    batch_size=3,\n",
    "    return_detokenized=True,\n",
    "    return_doc_details=True,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(\"Batches with document details:\")\n",
    "for batch_idx, batch in enumerate(batches_with_details):\n",
    "    batch_id = [1, 3][batch_idx]\n",
    "    print(f\"\\nBatch {batch_id}:\")\n",
    "    \n",
    "    for sample_idx, (text, doc_details) in enumerate(batch):\n",
    "        global_sample_id = batch_id * 3 + sample_idx\n",
    "        print(f\"  Sample {sample_idx} (global ID {global_sample_id}):\")\n",
    "        print(f\"    Length: {len(text)} chars\")\n",
    "        print(f\"    Doc range: {doc_details['doc_index_f']}-{doc_details['doc_index_l']}\")\n",
    "        print(f\"    Multi-doc: {doc_details['doc_index_f'] != doc_details['doc_index_l']}\")\n",
    "        print(f\"    Preview: {text[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eba13d",
   "metadata": {},
   "source": [
    "## Custom Sampling Policies\n",
    "\n",
    "The real power of TokenSmith's sampling comes from policy-based selection. You can define custom functions that determine which samples or batches to retrieve, enabling sophisticated sampling strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c730863",
   "metadata": {},
   "source": [
    "### Policy Functions for Individual Samples\n",
    "\n",
    "Let's start by creating various policy functions for individual sample selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57598d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined custom sampling policy functions!\n"
     ]
    }
   ],
   "source": [
    "# Define policy functions for sample selection\n",
    "\n",
    "def random_sample_policy(num_samples, max_index, rng_seed=42):\n",
    "    \"\"\"\n",
    "    Policy function that returns random sample indices.\n",
    "    \n",
    "    Args:\n",
    "        num_samples: Number of samples to return\n",
    "        max_index: Maximum index value (exclusive)\n",
    "        rng_seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        List of random sample indices\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(rng_seed)\n",
    "    return rng.integers(0, max_index, size=num_samples).tolist()\n",
    "\n",
    "def sequential_sample_policy(start_index, num_samples):\n",
    "    \"\"\"\n",
    "    Policy function that returns sequential sample indices.\n",
    "    \n",
    "    Args:\n",
    "        start_index: Starting index\n",
    "        num_samples: Number of consecutive samples to return\n",
    "    \n",
    "    Returns:\n",
    "        List of sequential sample indices\n",
    "    \"\"\"\n",
    "    return list(range(start_index, start_index + num_samples))\n",
    "\n",
    "def sparse_sample_policy(start_index, num_samples, step_size=10):\n",
    "    \"\"\"\n",
    "    Policy function that returns sparsely distributed sample indices.\n",
    "    \n",
    "    Args:\n",
    "        start_index: Starting index\n",
    "        num_samples: Number of samples to return\n",
    "        step_size: Step size between samples\n",
    "    \n",
    "    Returns:\n",
    "        List of sparse sample indices\n",
    "    \"\"\"\n",
    "    return [start_index + i * step_size for i in range(num_samples)]\n",
    "\n",
    "def prime_sample_policy(max_index):\n",
    "    \"\"\"\n",
    "    Policy function that returns sample indices at prime numbers.\n",
    "    \n",
    "    Args:\n",
    "        max_index: Maximum index to consider\n",
    "    \n",
    "    Returns:\n",
    "        List of prime-numbered sample indices\n",
    "    \"\"\"\n",
    "    def is_prime(n):\n",
    "        if n < 2:\n",
    "            return False\n",
    "        for i in range(2, int(n**0.5) + 1):\n",
    "            if n % i == 0:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    return [i for i in range(2, max_index) if is_prime(i)][:10]  # Limit to first 10 primes\n",
    "\n",
    "print(\"Defined custom sampling policy functions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a64c6f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Random Sample Policy Example ===\n",
      "Random Sample 1: 8936 chars\n",
      "  Text:  and Dad's hands. They were alone in the crowd. \"Mom! Dad! Where are you?\" Lily shouted. \"Help! Help...\n",
      "\n",
      "Random Sample 2: 8663 chars\n",
      "  Text:  He was so excited to tell her he was being patient! Mom was very proud of him and said he was growi...\n",
      "\n",
      "Random Sample 3: 8682 chars\n",
      "  Text: 's parents took him to the park and he saw the bird again. The old man was there as well. This time,...\n",
      "\n",
      "Random Sample 4: 8544 chars\n",
      "  Text:  let go of the car. \"I am sorry, Ben,\" Anna says. \"I am sorry, Anna,\" Ben says. They hug each other....\n",
      "\n",
      "Random Sample 5: 8654 chars\n",
      "  Text:  make a picture for his mom, who was at work. He took a big paper and some colors and started to dra...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Random sampling policy\n",
    "print(\"=== Random Sample Policy Example ===\")\n",
    "\n",
    "random_samples = dataset_manager.sample.get_samples_by_policy(\n",
    "    policy_fn=random_sample_policy,\n",
    "    num_samples=5,\n",
    "    max_index=200,  # Sample from first 200 indices\n",
    "    rng_seed=42,\n",
    "    return_detokenized=True,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "for i, sample in enumerate(random_samples):\n",
    "    print(f\"Random Sample {i+1}: {len(sample)} chars\")\n",
    "    print(f\"  Text: {sample[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fb2f71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sequential Sample Policy Example ===\n",
      "Sequential Sample 100: 8699 chars\n",
      "  Text:  and peeked over the grass to get a better look. Sally asked the sheep, “What’s your name?” The youn...\n",
      "\n",
      "Sequential Sample 101: 8723 chars\n",
      "  Text:  it.\" Lily nodded and put the pebble in her pocket. She played for a while and then went back inside...\n",
      "\n",
      "Sequential Sample 102: 8948 chars\n",
      "  Text:  better than the brick! Mandy was very pleased with her new snack. She ate the carrot and thanked th...\n",
      "\n",
      "Sequential Sample 103: 8639 chars\n",
      "  Text:  ran to the house. She was cold and sad. She wished she had listened to Tom. She reached the house a...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Sequential sampling policy\n",
    "print(\"=== Sequential Sample Policy Example ===\")\n",
    "\n",
    "sequential_samples = dataset_manager.sample.get_samples_by_policy(\n",
    "    policy_fn=sequential_sample_policy,\n",
    "    start_index=100,\n",
    "    num_samples=4,\n",
    "    return_detokenized=True,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "for i, sample in enumerate(sequential_samples):\n",
    "    sample_id = 100 + i\n",
    "    print(f\"Sequential Sample {sample_id}: {len(sample)} chars\")\n",
    "    print(f\"  Text: {sample[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ff7c02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sparse Sample Policy with Document Details ===\n",
      "Sparse Sample 50:\n",
      "  Length: 8495 chars\n",
      "  Document range: 14417 to 14425\n",
      "  Text:  Ben's car fell on the ground and broke. The wheel came off and the paint scratc...\n",
      "\n",
      "Sparse Sample 75:\n",
      "  Length: 8828 chars\n",
      "  Document range: 16055 to 16064\n",
      "  Text:  the museum. Sam was very happy that she was able to help BRO when he was broken...\n",
      "\n",
      "Sparse Sample 100:\n",
      "  Length: 8699 chars\n",
      "  Document range: 21659 to 21670\n",
      "  Text:  and peeked over the grass to get a better look. Sally asked the sheep, “What’s ...\n",
      "\n",
      "Sparse Sample 125:\n",
      "  Length: 8419 chars\n",
      "  Document range: 1534 to 1544\n",
      "  Text:  a time there was a little ice cream cone. It was filled with white, creamy ice ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Sparse sampling policy with document details\n",
    "print(\"=== Sparse Sample Policy with Document Details ===\")\n",
    "\n",
    "sparse_samples = dataset_manager.sample.get_samples_by_policy(\n",
    "    policy_fn=sparse_sample_policy,\n",
    "    start_index=50,\n",
    "    num_samples=4,\n",
    "    step_size=25,\n",
    "    return_doc_details=True,\n",
    "    return_detokenized=True,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "for i, (sample, doc_details) in enumerate(sparse_samples):\n",
    "    sample_id = 50 + i * 25\n",
    "    print(f\"Sparse Sample {sample_id}:\")\n",
    "    print(f\"  Length: {len(sample)} chars\")\n",
    "    print(f\"  Document range: {doc_details['doc_index_f']} to {doc_details['doc_index_l']}\")\n",
    "    print(f\"  Text: {sample[:80]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6fc922e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Prime Sample Policy Example ===\n",
      "Found 10 samples at prime indices:\n",
      "Prime Index 2: 8789 chars\n",
      "  Text:  agreed to marry him. They had a wonderful wedding and were very happy...\n",
      "Prime Index 3: 8700 chars\n",
      "  Text:  sleep, Maggie's mommy saw something very rare and wet. It was raining...\n",
      "Prime Index 5: 8404 chars\n",
      "  Text: Let's look at the pictures. They might tell us something.\" Lila and Be...\n",
      "Prime Index 7: 8719 chars\n",
      "  Text:  said. Bella was shocked and excited. She hugged her parents and thank...\n",
      "Prime Index 11: 8813 chars\n",
      "  Text:  and dad said it seemed too dangerous, so they said no. The boy didn't...\n",
      "  ... and 5 more\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Prime number sampling policy\n",
    "print(\"=== Prime Sample Policy Example ===\")\n",
    "\n",
    "prime_samples = dataset_manager.sample.get_samples_by_policy(\n",
    "    policy_fn=prime_sample_policy,\n",
    "    max_index=100,\n",
    "    return_detokenized=True,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(f\"Found {len(prime_samples)} samples at prime indices:\")\n",
    "prime_indices = prime_sample_policy(100)\n",
    "for i, (sample, prime_idx) in enumerate(zip(prime_samples, prime_indices)):\n",
    "    print(f\"Prime Index {prime_idx}: {len(sample)} chars\")\n",
    "    print(f\"  Text: {sample[:70]}...\")\n",
    "    if i >= 4:  # Show only first 5\n",
    "        print(f\"  ... and {len(prime_samples) - 5} more\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0987f15b",
   "metadata": {},
   "source": [
    "### Policy Functions for Batch Sampling\n",
    "\n",
    "Now let's create policy functions for batch selection, which can be useful for examining training dynamics or data distribution across batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc10bd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined custom batch sampling policy functions!\n"
     ]
    }
   ],
   "source": [
    "# Define policy functions for batch selection\n",
    "\n",
    "def random_batch_policy(num_batches, max_batch_id, rng_seed=42):\n",
    "    \"\"\"\n",
    "    Policy function that returns random batch IDs.\n",
    "    \n",
    "    Args:\n",
    "        num_batches: Number of batches to return\n",
    "        max_batch_id: Maximum batch ID value (exclusive)\n",
    "        rng_seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        List of random batch IDs\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(rng_seed)\n",
    "    return rng.integers(0, max_batch_id, size=num_batches).tolist()\n",
    "\n",
    "def sequential_batch_policy(start_batch_id, num_batches):\n",
    "    \"\"\"\n",
    "    Policy function that returns sequential batch IDs.\n",
    "    \n",
    "    Args:\n",
    "        start_batch_id: Starting batch ID\n",
    "        num_batches: Number of consecutive batches to return\n",
    "    \n",
    "    Returns:\n",
    "        List of sequential batch IDs\n",
    "    \"\"\"\n",
    "    return list(range(start_batch_id, start_batch_id + num_batches))\n",
    "\n",
    "def stride_batch_policy(start_batch_id, num_batches, stride=5):\n",
    "    \"\"\"\n",
    "    Policy function that returns batch IDs with a specific stride.\n",
    "    \n",
    "    Args:\n",
    "        start_batch_id: Starting batch ID\n",
    "        num_batches: Number of batches to return\n",
    "        stride: Stride between batch IDs\n",
    "    \n",
    "    Returns:\n",
    "        List of strided batch IDs\n",
    "    \"\"\"\n",
    "    return [start_batch_id + i * stride for i in range(num_batches)]\n",
    "\n",
    "def fibonacci_batch_policy(max_batch_id):\n",
    "    \"\"\"\n",
    "    Policy function that returns batch IDs at Fibonacci numbers.\n",
    "    \n",
    "    Args:\n",
    "        max_batch_id: Maximum batch ID to consider\n",
    "    \n",
    "    Returns:\n",
    "        List of Fibonacci-numbered batch IDs\n",
    "    \"\"\"\n",
    "    fib = [1, 1]\n",
    "    while fib[-1] < max_batch_id:\n",
    "        fib.append(fib[-1] + fib[-2])\n",
    "    \n",
    "    return [f for f in fib if f < max_batch_id][:8]  # Limit to first 8\n",
    "\n",
    "print(\"Defined custom batch sampling policy functions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6a58988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Random Batch Policy Example ===\n",
      "Random Batch 1:\n",
      "  Sample 1:  thing happened. Lily's little brother came running and acci...\n",
      "  Sample 2: . She had gone to the office for a minute. Lily had an idea....\n",
      "  Sample 3:  agreed to marry him. They had a wonderful wedding and were ...\n",
      "\n",
      "Random Batch 2:\n",
      "  Sample 1:  play is a good thing to do and it can make people happy.<|e...\n",
      "  Sample 2: . The ball is empty and flat. \"Oh no, we broke the ball!\" Li...\n",
      "  Sample 3:  dad said they could go and play on the sand and look for sh...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Random batch policy\n",
    "print(\"=== Random Batch Policy Example ===\")\n",
    "\n",
    "random_batches = dataset_manager.sample.get_batches_by_policy(\n",
    "    policy_fn=random_batch_policy,\n",
    "    batch_size=3,\n",
    "    num_batches=2,\n",
    "    max_batch_id=20,\n",
    "    rng_seed=123,\n",
    "    return_detokenized=True,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "for batch_idx, batch in enumerate(random_batches):\n",
    "    print(f\"Random Batch {batch_idx + 1}:\")\n",
    "    for sample_idx, sample in enumerate(batch):\n",
    "        print(f\"  Sample {sample_idx + 1}: {sample[:60]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40124bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sequential Batch Policy Example ===\n",
      "Sequential Batch 5:\n",
      "  Sample 1:  day, they find a big club on the grass. It is brown and hea...\n",
      "  Sample 2:  and dad said it seemed too dangerous, so they said no. The ...\n",
      "\n",
      "Sequential Batch 6:\n",
      "  Sample 1:  He thought it was a good idea to play under it. So Bob walk...\n",
      "  Sample 2:  dog named Spot. Spot liked nothing better than to play in t...\n",
      "\n",
      "Sequential Batch 7:\n",
      "  Sample 1: . She says: \"That is a bad man! He is not your friend! He wa...\n",
      "  Sample 2: ,\" she said with a big, happy grin. Dad smiled and said, \"Ye...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Sequential batch policy\n",
    "print(\"=== Sequential Batch Policy Example ===\")\n",
    "\n",
    "sequential_batches = dataset_manager.sample.get_batches_by_policy(\n",
    "    policy_fn=sequential_batch_policy,\n",
    "    batch_size=2,\n",
    "    start_batch_id=5,\n",
    "    num_batches=3,\n",
    "    return_detokenized=True,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "for batch_idx, batch in enumerate(sequential_batches):\n",
    "    batch_id = 5 + batch_idx\n",
    "    print(f\"Sequential Batch {batch_id}:\")\n",
    "    for sample_idx, sample in enumerate(batch):\n",
    "        print(f\"  Sample {sample_idx + 1}: {sample[:60]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8d70272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Stride Batch Policy with Document Details ===\n",
      "Stride Batch 3:\n",
      "  Sample 1:\n",
      "    Doc range: 18928-18938\n",
      "    Text:  baby, saving them from certain injury. The creati...\n",
      "  Sample 2:\n",
      "    Doc range: 7508-7515\n",
      "    Text:  said. Bella was shocked and excited. She hugged h...\n",
      "\n",
      "Stride Batch 7:\n",
      "  Sample 1:\n",
      "    Doc range: 16979-16987\n",
      "    Text: . She says: \"That is a bad man! He is not your fri...\n",
      "  Sample 2:\n",
      "    Doc range: 4665-4676\n",
      "    Text: ,\" she said with a big, happy grin. Dad smiled and...\n",
      "\n",
      "Stride Batch 11:\n",
      "  Sample 1:\n",
      "    Doc range: 15364-15375\n",
      "    Text:  Suddenly, a big, brown pony appeared in front of ...\n",
      "  Sample 2:\n",
      "    Doc range: 14762-14772\n",
      "    Text:  also about being kind and working hard. From that...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Stride batch policy with document details\n",
    "print(\"=== Stride Batch Policy with Document Details ===\")\n",
    "\n",
    "stride_batches = dataset_manager.sample.get_batches_by_policy(\n",
    "    policy_fn=stride_batch_policy,\n",
    "    batch_size=2,\n",
    "    start_batch_id=3,\n",
    "    num_batches=3,\n",
    "    stride=4,  # Skip 3 batches between selections\n",
    "    return_doc_details=True,\n",
    "    return_detokenized=True,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "for batch_idx, batch in enumerate(stride_batches):\n",
    "    batch_id = 3 + batch_idx * 4\n",
    "    print(f\"Stride Batch {batch_id}:\")\n",
    "    for sample_idx, (sample, doc_details) in enumerate(batch):\n",
    "        print(f\"  Sample {sample_idx + 1}:\")\n",
    "        print(f\"    Doc range: {doc_details['doc_index_f']}-{doc_details['doc_index_l']}\")\n",
    "        print(f\"    Text: {sample[:50]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "610078cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fibonacci Batch Policy Example ===\n",
      "Fibonacci batch IDs: [1, 1, 2, 3, 5, 8, 13, 21]\n",
      "Fibonacci Batch 1:\n",
      "  Sample 1:  agreed to marry him. They had a wonderful wedding...\n",
      "  Sample 2:  sleep, Maggie's mommy saw something very rare and...\n",
      "\n",
      "Fibonacci Batch 1:\n",
      "  Sample 1:  agreed to marry him. They had a wonderful wedding...\n",
      "  Sample 2:  sleep, Maggie's mommy saw something very rare and...\n",
      "\n",
      "Fibonacci Batch 2:\n",
      "  Sample 1:  Benny learned that not all big, wild animals are ...\n",
      "  Sample 2: Let's look at the pictures. They might tell us som...\n",
      "\n",
      "... and 5 more batches\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Fibonacci batch policy\n",
    "print(\"=== Fibonacci Batch Policy Example ===\")\n",
    "\n",
    "fib_batches = dataset_manager.sample.get_batches_by_policy(\n",
    "    policy_fn=fibonacci_batch_policy,\n",
    "    batch_size=2,\n",
    "    max_batch_id=25,\n",
    "    return_detokenized=True,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "fib_batch_ids = fibonacci_batch_policy(25)\n",
    "print(f\"Fibonacci batch IDs: {fib_batch_ids}\")\n",
    "\n",
    "for batch_idx, batch in enumerate(fib_batches[:3]):  # Show first 3\n",
    "    batch_id = fib_batch_ids[batch_idx]\n",
    "    print(f\"Fibonacci Batch {batch_id}:\")\n",
    "    for sample_idx, sample in enumerate(batch):\n",
    "        print(f\"  Sample {sample_idx + 1}: {sample[:50]}...\")\n",
    "    print()\n",
    "\n",
    "if len(fib_batches) > 3:\n",
    "    print(f\"... and {len(fib_batches) - 3} more batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53dd824",
   "metadata": {},
   "source": [
    "## Advanced Policy Examples\n",
    "\n",
    "Let's explore some more sophisticated sampling policies that might be useful for research and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c7e85ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Length-Based Sample Policy Example ===\n",
      "Target length 7500, actual length 8195 (diff: 695):\n",
      "  Text: Lily liked to gather bugs in her jar. She would look for them in the grass, unde...\n",
      "\n",
      "Target length 8500, actual length 8500 (diff: 0):\n",
      "  Text:  and dirt, and soon the necklace was clean and light. Jim was so happy, he hugge...\n",
      "\n",
      "Target length 9000, actual length 8998 (diff: 2):\n",
      "  Text:  also about being kind and working hard. From that day on, Timmy worked hard and...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Advanced policy: Sample based on text length distribution\n",
    "def length_based_sample_policy(dataset_manager, tokenizer_for_policy, num_samples=5, target_lengths=None):\n",
    "    \"\"\"\n",
    "    Policy that samples based on desired text lengths.\n",
    "    \n",
    "    This is a more complex policy that examines actual samples to find ones\n",
    "    matching certain criteria.\n",
    "    \"\"\"\n",
    "    if target_lengths is None:\n",
    "        target_lengths = [7000, 8000, 8500, 9000, 9500]  # Different length targets\n",
    "    \n",
    "    selected_indices = []\n",
    "    search_range = min(500, num_samples * 50)  # Reasonable search space\n",
    "    \n",
    "    for target_length in target_lengths[:num_samples]:\n",
    "        best_idx = None\n",
    "        best_diff = float('inf')\n",
    "        \n",
    "        # Search through a range of samples\n",
    "        for idx in range(search_range):\n",
    "            try:\n",
    "                # Get sample text to check length\n",
    "                text = dataset_manager.sample.get_samples_by_indices(\n",
    "                    indices=[idx],\n",
    "                    return_detokenized=True,\n",
    "                    tokenizer=tokenizer_for_policy\n",
    "                )[0]\n",
    "                \n",
    "                diff = abs(len(text) - target_length)\n",
    "                if diff < best_diff:\n",
    "                    best_diff = diff\n",
    "                    best_idx = idx\n",
    "                    \n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        if best_idx is not None:\n",
    "            selected_indices.append(best_idx)\n",
    "    \n",
    "    return selected_indices\n",
    "\n",
    "# Use the advanced policy\n",
    "print(\"=== Length-Based Sample Policy Example ===\")\n",
    "\n",
    "length_samples = dataset_manager.sample.get_samples_by_policy(\n",
    "    policy_fn=length_based_sample_policy,\n",
    "    dataset_manager=dataset_manager,\n",
    "    tokenizer_for_policy=tokenizer,\n",
    "    num_samples=3,\n",
    "    target_lengths=[7500, 8500, 9000],\n",
    "    return_detokenized=True,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "target_lengths = [7500, 8500, 9000]\n",
    "for i, sample in enumerate(length_samples):\n",
    "    target = target_lengths[i]\n",
    "    actual = len(sample)\n",
    "    print(f\"Target length {target}, actual length {actual} (diff: {abs(actual-target)}):\")\n",
    "    print(f\"  Text: {sample[:80]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eff58427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Lambda Policy Functions Example ===\n",
      "Samples at perfect square indices:\n",
      "Square Index 1: . She had gone to the office for a minute. Lily had an idea....\n",
      "Square Index 4:  Benny learned that not all big, wild animals are scary. Som...\n",
      "Square Index 9:  run around the faucet, letting the water spray all over him...\n",
      "Square Index 16:  you too,\" she says. \"But you have to promise me that you wo...\n",
      "Square Index 25:  that the sun made droplets scatter off of their backs! They...\n",
      "\n",
      "==================================================\n",
      "Batches at power-of-2 IDs:\n",
      "Batch ID 2:\n",
      "  Sample 1:  Benny learned that not all big, wild animals are ...\n",
      "  Sample 2: Let's look at the pictures. They might tell us som...\n",
      "\n",
      "Batch ID 4:\n",
      "  Sample 1:  park. They see the slide and the swing. They wish...\n",
      "  Sample 2:  run around the faucet, letting the water spray al...\n",
      "\n",
      "Batch ID 8:\n",
      "  Sample 1:  you too,\" she says. \"But you have to promise me t...\n",
      "  Sample 2:  and Dad's hands. They were alone in the crowd. \"M...\n",
      "\n",
      "Batch ID 16:\n",
      "  Sample 1: my took him to the airport to see a famous plane. ...\n",
      "  Sample 2:  off. The man finally smiled back at the little gi...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Advanced policy: Lambda functions for complex logic\n",
    "print(\"=== Lambda Policy Functions Example ===\")\n",
    "\n",
    "# Example 1: Sample indices that are perfect squares\n",
    "square_samples = dataset_manager.sample.get_samples_by_policy(\n",
    "    policy_fn=lambda max_idx: [i*i for i in range(1, int(max_idx**0.5)+1) if i*i < max_idx][:5],\n",
    "    max_idx=100,\n",
    "    return_detokenized=True,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "square_indices = [i*i for i in range(1, int(100**0.5)+1) if i*i < 100][:5]\n",
    "print(\"Samples at perfect square indices:\")\n",
    "for i, (sample, sq_idx) in enumerate(zip(square_samples, square_indices)):\n",
    "    print(f\"Square Index {sq_idx}: {sample[:60]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Example 2: Batch IDs that are powers of 2\n",
    "power_batches = dataset_manager.sample.get_batches_by_policy(\n",
    "    policy_fn=lambda max_power: [2**i for i in range(1, max_power+1)],\n",
    "    batch_size=2,\n",
    "    max_power=4,  # 2^1, 2^2, 2^3, 2^4 = batches 2, 4, 8, 16\n",
    "    return_detokenized=True,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "power_batch_ids = [2**i for i in range(1, 5)]\n",
    "print(\"Batches at power-of-2 IDs:\")\n",
    "for batch_idx, batch in enumerate(power_batches):\n",
    "    batch_id = power_batch_ids[batch_idx]\n",
    "    print(f\"Batch ID {batch_id}:\")\n",
    "    for sample_idx, sample in enumerate(batch):\n",
    "        print(f\"  Sample {sample_idx + 1}: {sample[:50]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a8be9e",
   "metadata": {},
   "source": [
    "## Practical Sampling Utilities\n",
    "\n",
    "Let's create some utility functions that demonstrate practical applications of sampling for dataset analysis and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3af5c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset Quality Check Example ===\n",
      "Quality check results (15 samples):\n",
      "  Average length: 8713.5 ± 175.3 chars\n",
      "  Length range: 8173 - 8938 chars\n",
      "  Multi-document samples: 15 (100.0%)\n",
      "  Average document span: 11.1 documents\n",
      "  Document span range: 9 - 13 documents\n"
     ]
    }
   ],
   "source": [
    "def sample_quality_check(dataset_manager, tokenizer, num_samples=10, seed=42):\n",
    "    \"\"\"\n",
    "    Utility function to perform quality checks on random samples.\n",
    "    \n",
    "    Returns statistics about sample lengths, document boundaries, etc.\n",
    "    \"\"\"\n",
    "    # Get random samples with document details\n",
    "    samples = dataset_manager.sample.get_samples_by_policy(\n",
    "        policy_fn=random_sample_policy,\n",
    "        num_samples=num_samples,\n",
    "        max_index=500,\n",
    "        rng_seed=seed,\n",
    "        return_doc_details=True,\n",
    "        return_detokenized=True,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    stats = {\n",
    "        'num_samples': len(samples),\n",
    "        'lengths': [],\n",
    "        'multi_doc_count': 0,\n",
    "        'doc_spans': [],\n",
    "        'avg_length': 0,\n",
    "        'length_std': 0\n",
    "    }\n",
    "    \n",
    "    for text, doc_details in samples:\n",
    "        length = len(text)\n",
    "        stats['lengths'].append(length)\n",
    "        \n",
    "        doc_span = doc_details['doc_index_l'] - doc_details['doc_index_f'] + 1\n",
    "        stats['doc_spans'].append(doc_span)\n",
    "        \n",
    "        if doc_details['doc_index_f'] != doc_details['doc_index_l']:\n",
    "            stats['multi_doc_count'] += 1\n",
    "    \n",
    "    stats['avg_length'] = np.mean(stats['lengths'])\n",
    "    stats['length_std'] = np.std(stats['lengths'])\n",
    "    stats['avg_doc_span'] = np.mean(stats['doc_spans'])\n",
    "    stats['multi_doc_percentage'] = (stats['multi_doc_count'] / num_samples) * 100\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Run quality check\n",
    "print(\"=== Dataset Quality Check Example ===\")\n",
    "\n",
    "quality_stats = sample_quality_check(dataset_manager, tokenizer, num_samples=15)\n",
    "\n",
    "print(f\"Quality check results ({quality_stats['num_samples']} samples):\")\n",
    "print(f\"  Average length: {quality_stats['avg_length']:.1f} ± {quality_stats['length_std']:.1f} chars\")\n",
    "print(f\"  Length range: {min(quality_stats['lengths'])} - {max(quality_stats['lengths'])} chars\")\n",
    "print(f\"  Multi-document samples: {quality_stats['multi_doc_count']} ({quality_stats['multi_doc_percentage']:.1f}%)\")\n",
    "print(f\"  Average document span: {quality_stats['avg_doc_span']:.1f} documents\")\n",
    "print(f\"  Document span range: {min(quality_stats['doc_spans'])} - {max(quality_stats['doc_spans'])} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b943fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sampling Strategy Comparison ===\n",
      "Random sampling:\n",
      "  Indices: [17, 154, 130, 87, 86]\n",
      "  Avg length: 8695.8 ± 129.4\n",
      "  Multi-doc %: 100.0%\n",
      "\n",
      "Sequential sampling:\n",
      "  Indices: [100, 101, 102, 103, 104]\n",
      "  Avg length: 8739.8 ± 107.6\n",
      "  Multi-doc %: 100.0%\n",
      "\n",
      "Sparse sampling:\n",
      "  Indices: [50, 70, 90, 110, 130]\n",
      "  Avg length: 8708.8 ± 154.4\n",
      "  Multi-doc %: 100.0%\n",
      "\n",
      "Prime sampling:\n",
      "  Indices: [2, 3, 5, 7, 11]\n",
      "  Avg length: 8685.0 ± 146.7\n",
      "  Multi-doc %: 100.0%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compare_sampling_strategies(dataset_manager, tokenizer):\n",
    "    \"\"\"\n",
    "    Compare different sampling strategies to understand their characteristics.\n",
    "    \"\"\"\n",
    "    strategies = {\n",
    "        'Random': lambda: random_sample_policy(5, 200, 42),\n",
    "        'Sequential': lambda: sequential_sample_policy(100, 5),\n",
    "        'Sparse': lambda: sparse_sample_policy(50, 5, 20),\n",
    "        'Prime': lambda: prime_sample_policy(50)[:5]\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for strategy_name, policy_fn in strategies.items():\n",
    "        indices = policy_fn()\n",
    "        samples = dataset_manager.sample.get_samples_by_indices(\n",
    "            indices=indices,\n",
    "            return_detokenized=True,\n",
    "            return_doc_details=True,\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "        \n",
    "        lengths = [len(text) for text, _ in samples]\n",
    "        multi_doc_count = sum(1 for _, doc_details in samples \n",
    "                             if doc_details['doc_index_f'] != doc_details['doc_index_l'])\n",
    "        \n",
    "        results[strategy_name] = {\n",
    "            'indices': indices,\n",
    "            'avg_length': np.mean(lengths),\n",
    "            'length_std': np.std(lengths),\n",
    "            'multi_doc_percentage': (multi_doc_count / len(samples)) * 100\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Compare strategies\n",
    "print(\"=== Sampling Strategy Comparison ===\")\n",
    "\n",
    "comparison = compare_sampling_strategies(dataset_manager, tokenizer)\n",
    "\n",
    "for strategy, stats in comparison.items():\n",
    "    print(f\"{strategy} sampling:\")\n",
    "    print(f\"  Indices: {stats['indices']}\")\n",
    "    print(f\"  Avg length: {stats['avg_length']:.1f} ± {stats['length_std']:.1f}\")\n",
    "    print(f\"  Multi-doc %: {stats['multi_doc_percentage']:.1f}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bfef4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Batch Diversity Analysis ===\n",
      "Batch-by-batch analysis:\n",
      "  Batch 2:\n",
      "    Avg length: 8605.3\n",
      "    Length variance: 6461.6\n",
      "    Multi-doc samples: 3\n",
      "  Batch 4:\n",
      "    Avg length: 8614.0\n",
      "    Length variance: 66940.7\n",
      "    Multi-doc samples: 3\n",
      "  Batch 6:\n",
      "    Avg length: 8777.3\n",
      "    Length variance: 20437.6\n",
      "    Multi-doc samples: 3\n",
      "  Batch 8:\n",
      "    Avg length: 8395.0\n",
      "    Length variance: 20778.0\n",
      "    Multi-doc samples: 3\n",
      "\n",
      "Overall statistics:\n",
      "  Overall length variance: 47074.2\n",
      "  Cross-batch variance: 18419.8\n",
      "  Diversity ratio: 0.391\n"
     ]
    }
   ],
   "source": [
    "def batch_diversity_analysis(dataset_manager, tokenizer, batch_ids, batch_size):\n",
    "    \"\"\"\n",
    "    Analyze diversity within and across batches.\n",
    "    \"\"\"\n",
    "    batches = dataset_manager.sample.get_batches_by_ids(\n",
    "        batch_ids=batch_ids,\n",
    "        batch_size=batch_size,\n",
    "        return_detokenized=True,\n",
    "        return_doc_details=True,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    analysis = {\n",
    "        'batch_stats': [],\n",
    "        'overall_length_variance': 0,\n",
    "        'cross_batch_variance': 0\n",
    "    }\n",
    "    \n",
    "    all_lengths = []\n",
    "    batch_avg_lengths = []\n",
    "    \n",
    "    for batch_idx, batch in enumerate(batches):\n",
    "        batch_id = batch_ids[batch_idx]\n",
    "        lengths = [len(text) for text, _ in batch]\n",
    "        multi_docs = [1 for _, doc_details in batch \n",
    "                     if doc_details['doc_index_f'] != doc_details['doc_index_l']]\n",
    "        \n",
    "        batch_stat = {\n",
    "            'batch_id': batch_id,\n",
    "            'avg_length': np.mean(lengths),\n",
    "            'length_variance': np.var(lengths),\n",
    "            'multi_doc_count': sum(multi_docs)\n",
    "        }\n",
    "        \n",
    "        analysis['batch_stats'].append(batch_stat)\n",
    "        all_lengths.extend(lengths)\n",
    "        batch_avg_lengths.append(batch_stat['avg_length'])\n",
    "    \n",
    "    analysis['overall_length_variance'] = np.var(all_lengths)\n",
    "    analysis['cross_batch_variance'] = np.var(batch_avg_lengths)\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Analyze batch diversity\n",
    "print(\"=== Batch Diversity Analysis ===\")\n",
    "\n",
    "diversity_analysis = batch_diversity_analysis(\n",
    "    dataset_manager, tokenizer, \n",
    "    batch_ids=[2, 4, 6, 8], \n",
    "    batch_size=3\n",
    ")\n",
    "\n",
    "print(\"Batch-by-batch analysis:\")\n",
    "for stats in diversity_analysis['batch_stats']:\n",
    "    print(f\"  Batch {stats['batch_id']}:\")\n",
    "    print(f\"    Avg length: {stats['avg_length']:.1f}\")\n",
    "    print(f\"    Length variance: {stats['length_variance']:.1f}\")\n",
    "    print(f\"    Multi-doc samples: {stats['multi_doc_count']}\")\n",
    "\n",
    "print(f\"\\nOverall statistics:\")\n",
    "print(f\"  Overall length variance: {diversity_analysis['overall_length_variance']:.1f}\")\n",
    "print(f\"  Cross-batch variance: {diversity_analysis['cross_batch_variance']:.1f}\")\n",
    "print(f\"  Diversity ratio: {diversity_analysis['cross_batch_variance'] / diversity_analysis['overall_length_variance']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4981db25",
   "metadata": {},
   "source": [
    "## Performance Considerations\n",
    "\n",
    "When working with large datasets, it's important to understand the performance characteristics of different sampling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40c8532b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Performance Benchmarks ===\n",
      "Sampling method performance (10 samples each):\n",
      "  Index-based: 0.0183 seconds\n",
      "  Policy-based (simple): 0.0174 seconds\n",
      "  Batch sampling: 0.0174 seconds\n",
      "  With doc details: 0.0191 seconds\n",
      "\n",
      "Relative to fastest method:\n",
      "  Index-based: 1.05x\n",
      "  Policy-based (simple): 1.00x\n",
      "  Batch sampling: 1.00x\n",
      "  With doc details: 1.10x\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_sampling_methods(dataset_manager, tokenizer):\n",
    "    \"\"\"\n",
    "    Benchmark different sampling methods to understand their performance.\n",
    "    \"\"\"\n",
    "    benchmarks = {}\n",
    "    \n",
    "    # Test 1: Index-based sampling\n",
    "    start_time = time.time()\n",
    "    indices = list(range(0, 50, 5))  # Every 5th sample from 0 to 50\n",
    "    samples = dataset_manager.sample.get_samples_by_indices(\n",
    "        indices=indices,\n",
    "        return_detokenized=True,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    benchmarks['Index-based'] = time.time() - start_time\n",
    "    \n",
    "    # Test 2: Policy-based sampling (simple)\n",
    "    start_time = time.time()\n",
    "    samples = dataset_manager.sample.get_samples_by_policy(\n",
    "        policy_fn=lambda: list(range(0, 50, 5)),\n",
    "        return_detokenized=True,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    benchmarks['Policy-based (simple)'] = time.time() - start_time\n",
    "    \n",
    "    # Test 3: Batch sampling\n",
    "    start_time = time.time()\n",
    "    batches = dataset_manager.sample.get_batches_by_ids(\n",
    "        batch_ids=[0, 1, 2, 3, 4],\n",
    "        batch_size=2,\n",
    "        return_detokenized=True,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    benchmarks['Batch sampling'] = time.time() - start_time\n",
    "    \n",
    "    # Test 4: With document details\n",
    "    start_time = time.time()\n",
    "    samples = dataset_manager.sample.get_samples_by_indices(\n",
    "        indices=indices,\n",
    "        return_detokenized=True,\n",
    "        return_doc_details=True,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    benchmarks['With doc details'] = time.time() - start_time\n",
    "    \n",
    "    return benchmarks\n",
    "\n",
    "# Run benchmarks\n",
    "print(\"=== Performance Benchmarks ===\")\n",
    "\n",
    "benchmarks = benchmark_sampling_methods(dataset_manager, tokenizer)\n",
    "\n",
    "print(\"Sampling method performance (10 samples each):\")\n",
    "for method, duration in benchmarks.items():\n",
    "    print(f\"  {method}: {duration:.4f} seconds\")\n",
    "\n",
    "# Calculate relative performance\n",
    "fastest = min(benchmarks.values())\n",
    "print(f\"\\nRelative to fastest method:\")\n",
    "for method, duration in benchmarks.items():\n",
    "    relative = duration / fastest\n",
    "    print(f\"  {method}: {relative:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b1271c",
   "metadata": {},
   "source": [
    "## Best Practices and Tips\n",
    "\n",
    "Here are some practical tips for effective sampling with TokenSmith:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c070164d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Efficient Large-Scale Sampling ===\n",
      "Processed 50 samples in 0.0913 seconds\n",
      "Rate: 547.8 samples/second\n",
      "Length statistics: 8668.9 ± 211.0\n"
     ]
    }
   ],
   "source": [
    "# Tip 1: Efficient batch processing for large-scale analysis\n",
    "def efficient_large_scale_sampling(dataset_manager, tokenizer, total_samples=100):\n",
    "    \"\"\"\n",
    "    Demonstrate efficient sampling for large-scale analysis.\n",
    "    \"\"\"\n",
    "    print(\"=== Efficient Large-Scale Sampling ===\")\n",
    "    \n",
    "    # Instead of sampling one by one, use batch-based approaches\n",
    "    batch_size = 10\n",
    "    num_batches = total_samples // batch_size\n",
    "    \n",
    "    # Use policy to get batch IDs efficiently\n",
    "    batch_ids = list(range(0, num_batches))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    all_batches = dataset_manager.sample.get_batches_by_ids(\n",
    "        batch_ids=batch_ids,\n",
    "        batch_size=batch_size,\n",
    "        return_detokenized=True,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    # Flatten for analysis\n",
    "    all_samples = [sample for batch in all_batches for sample in batch]\n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    print(f\"Processed {len(all_samples)} samples in {duration:.4f} seconds\")\n",
    "    print(f\"Rate: {len(all_samples)/duration:.1f} samples/second\")\n",
    "    \n",
    "    # Quick statistics\n",
    "    lengths = [len(sample) for sample in all_samples]\n",
    "    print(f\"Length statistics: {np.mean(lengths):.1f} ± {np.std(lengths):.1f}\")\n",
    "    \n",
    "    return all_samples\n",
    "\n",
    "# Demonstrate efficient sampling\n",
    "efficient_samples = efficient_large_scale_sampling(dataset_manager, tokenizer, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55e6d3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Reproducible Sampling Demo ===\n",
      "Samples identical across runs: True\n",
      "Different seed gives different samples: True\n"
     ]
    }
   ],
   "source": [
    "# Tip 2: Reproducible sampling with seeds\n",
    "def reproducible_sampling_demo(dataset_manager, tokenizer):\n",
    "    \"\"\"\n",
    "    Demonstrate reproducible sampling using seeds.\n",
    "    \"\"\"\n",
    "    print(\"=== Reproducible Sampling Demo ===\")\n",
    "    \n",
    "    # Same seed should give same results\n",
    "    seed = 12345\n",
    "    \n",
    "    # First run\n",
    "    samples1 = dataset_manager.sample.get_samples_by_policy(\n",
    "        policy_fn=random_sample_policy,\n",
    "        num_samples=5,\n",
    "        max_index=100,\n",
    "        rng_seed=seed,\n",
    "        return_detokenized=True,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    # Second run with same seed\n",
    "    samples2 = dataset_manager.sample.get_samples_by_policy(\n",
    "        policy_fn=random_sample_policy,\n",
    "        num_samples=5,\n",
    "        max_index=100,\n",
    "        rng_seed=seed,\n",
    "        return_detokenized=True,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    # Check if they're identical\n",
    "    identical = all(s1 == s2 for s1, s2 in zip(samples1, samples2))\n",
    "    print(f\"Samples identical across runs: {identical}\")\n",
    "    \n",
    "    # Different seed should give different results\n",
    "    samples3 = dataset_manager.sample.get_samples_by_policy(\n",
    "        policy_fn=random_sample_policy,\n",
    "        num_samples=5,\n",
    "        max_index=100,\n",
    "        rng_seed=seed + 1,\n",
    "        return_detokenized=True,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    different = any(s1 != s3 for s1, s3 in zip(samples1, samples3))\n",
    "    print(f\"Different seed gives different samples: {different}\")\n",
    "\n",
    "reproducible_sampling_demo(dataset_manager, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab01360b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Memory-Efficient Analysis ===\n",
      "Processed 40 samples in chunks of 10\n",
      "Average length: 8687.5 characters\n",
      "Multi-document samples: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Tip 3: Memory-efficient sampling for very large datasets\n",
    "def memory_efficient_analysis(dataset_manager, tokenizer, sample_indices):\n",
    "    \"\"\"\n",
    "    Demonstrate memory-efficient analysis techniques.\n",
    "    \"\"\"\n",
    "    print(\"=== Memory-Efficient Analysis ===\")\n",
    "    \n",
    "    # Process samples in chunks to avoid loading everything at once\n",
    "    chunk_size = 10\n",
    "    total_length = 0\n",
    "    multi_doc_count = 0\n",
    "    processed_count = 0\n",
    "    \n",
    "    for i in range(0, len(sample_indices), chunk_size):\n",
    "        chunk_indices = sample_indices[i:i + chunk_size]\n",
    "        \n",
    "        # Process chunk\n",
    "        chunk_samples = dataset_manager.sample.get_samples_by_indices(\n",
    "            indices=chunk_indices,\n",
    "            return_detokenized=True,\n",
    "            return_doc_details=True,\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "        \n",
    "        # Accumulate statistics without storing all data\n",
    "        for text, doc_details in chunk_samples:\n",
    "            total_length += len(text)\n",
    "            if doc_details['doc_index_f'] != doc_details['doc_index_l']:\n",
    "                multi_doc_count += 1\n",
    "            processed_count += 1\n",
    "        \n",
    "        # Clear chunk from memory (in real scenarios, this happens automatically)\n",
    "        del chunk_samples\n",
    "    \n",
    "    # Final statistics\n",
    "    avg_length = total_length / processed_count\n",
    "    multi_doc_percentage = (multi_doc_count / processed_count) * 100\n",
    "    \n",
    "    print(f\"Processed {processed_count} samples in chunks of {chunk_size}\")\n",
    "    print(f\"Average length: {avg_length:.1f} characters\")\n",
    "    print(f\"Multi-document samples: {multi_doc_percentage:.1f}%\")\n",
    "\n",
    "# Demonstrate memory-efficient processing\n",
    "large_sample_indices = list(range(0, 80, 2))  # Every other sample from 0 to 80\n",
    "memory_efficient_analysis(dataset_manager, tokenizer, large_sample_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18be09f4",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've successfully learned how to use TokenSmith's flexible sampling capabilities. Here's what we covered:\n",
    "\n",
    "### Key Concepts Learned:\n",
    "1. **Index-based Sampling**: Direct sampling by specifying exact sample indices\n",
    "2. **Batch Sampling**: Retrieving complete training batches by their IDs\n",
    "3. **Policy-based Sampling**: Using custom functions to define sampling strategies\n",
    "4. **Advanced Policies**: Creating sophisticated sampling logic for research needs\n",
    "5. **Performance Optimization**: Understanding trade-offs and efficiency considerations\n",
    "6. **Best Practices**: Reproducible, memory-efficient, and scalable sampling techniques\n",
    "\n",
    "### Key Methods Used:\n",
    "- `dataset_manager.sample.get_samples_by_indices()` - Sample by specific indices\n",
    "- `dataset_manager.sample.get_batches_by_ids()` - Sample complete batches\n",
    "- `dataset_manager.sample.get_samples_by_policy()` - Policy-based sample selection\n",
    "- `dataset_manager.sample.get_batches_by_policy()` - Policy-based batch selection\n",
    "\n",
    "### Sampling Strategies Explored:\n",
    "- **Random sampling**: For unbiased dataset exploration\n",
    "- **Sequential sampling**: For examining consecutive samples\n",
    "- **Sparse sampling**: For distributed dataset coverage\n",
    "- **Mathematical patterns**: Prime numbers, Fibonacci, powers of 2\n",
    "- **Length-based sampling**: Targeting specific text characteristics\n",
    "- **Custom lambda policies**: Flexible, inline sampling logic\n",
    "\n",
    "### Performance Insights:\n",
    "- Index-based sampling is fastest for known indices\n",
    "- Policy-based sampling adds minimal overhead for simple policies\n",
    "- Batch sampling is efficient for processing multiple samples\n",
    "- Document details add some processing cost but provide valuable metadata\n",
    "- Memory-efficient chunking enables large-scale analysis\n",
    "\n",
    "### Next Steps:\n",
    "- Tutorial 4: Explore search functionality across your dataset\n",
    "- Tutorial 5: Learn about dataset editing and injection capabilities\n",
    "- Tutorial 6: Export sampled data in various formats\n",
    "\n",
    "### Pro Tips:\n",
    "- Use seeds for reproducible sampling in research\n",
    "- Prefer batch-based operations for large-scale analysis\n",
    "- Create reusable policy functions for common sampling patterns\n",
    "- Consider memory usage when processing large numbers of samples\n",
    "- Combine sampling with inspection for comprehensive dataset understanding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neox_updated_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
