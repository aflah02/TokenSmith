{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f9a509f",
   "metadata": {},
   "source": [
    "# Search Functionality Tutorial\n",
    "\n",
    "Welcome to the comprehensive search functionality tutorial! This guide covers TokenSmith's powerful search capabilities for finding and analyzing token sequences in your datasets.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Basic search operations (count, contains, positions)\n",
    "- Advanced search features (next token prediction)\n",
    "- Batch search operations for efficiency\n",
    "- N-gram sampling with smoothing\n",
    "- Real-world search applications\n",
    "\n",
    "**Prerequisites:**\n",
    "- Completed basic setup tutorial\n",
    "- Understanding of tokenization\n",
    "- Familiarity with token sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa2ac19",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "First, let's set up our environment with the necessary imports and initialize our dataset manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcda5a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /NS/venvs/work/afkhan/neox_updated_env/lib/python3.11/site-packages (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /NS/venvs/work/afkhan/neox_updated_env/lib/python3.11/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /NS/venvs/work/afkhan/neox_updated_env/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /NS/venvs/work/afkhan/neox_updated_env/lib/python3.11/site-packages (from matplotlib) (4.58.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /NS/venvs/work/afkhan/neox_updated_env/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /NS/venvs/work/afkhan/neox_updated_env/lib/python3.11/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /NS/venvs/work/afkhan/neox_updated_env/lib/python3.11/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /NS/venvs/work/afkhan/neox_updated_env/lib/python3.11/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /NS/venvs/work/afkhan/neox_updated_env/lib/python3.11/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /NS/venvs/work/afkhan/neox_updated_env/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /NS/venvs/work/afkhan/neox_updated_env/lib/python3.11/site-packages (from matplotlib) (4.58.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /NS/venvs/work/afkhan/neox_updated_env/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /NS/venvs/work/afkhan/neox_updated_env/lib/python3.11/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /NS/venvs/work/afkhan/neox_updated_env/lib/python3.11/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /NS/venvs/work/afkhan/neox_updated_env/lib/python3.11/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /NS/venvs/work/afkhan/neox_updated_env/lib/python3.11/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /NS/venvs/work/afkhan/neox_updated_env/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /NS/venvs/work/afkhan/neox_updated_env/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /NS/venvs/work/afkhan/neox_updated_env/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "609c8907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# May not be necessary, but ensures the path is set correctly\n",
    "import sys\n",
    "sys.path.insert(0, \"/NS/llm-pretraining/work/afkhan/tokensmith\")\n",
    "sys.path.insert(0, \"/NS/llm-pretraining/work/afkhan/USC_Colab/gpt-neox\")\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2c8f3b",
   "metadata": {},
   "source": [
    "### Initialize Tokenizer\n",
    "\n",
    "For search operations, we need a tokenizer to convert text to tokens and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67d05c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/NS/venvs/work/afkhan/neox_updated_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: EleutherAI/gpt-neox-20b\n",
      "Vocabulary size: 50277\n",
      "EOS token: <|endoftext|> (ID: 0)\n",
      "BOS token: <|endoftext|> (ID: 0)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKENIZER_NAME_OR_PATH = \"EleutherAI/gpt-neox-20b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME_OR_PATH, add_eos_token=True)\n",
    "\n",
    "print(f\"Tokenizer loaded: {TOKENIZER_NAME_OR_PATH}\")\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "print(f\"BOS token: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95a1a0e",
   "metadata": {},
   "source": [
    "### Setup Dataset Manager\n",
    "\n",
    "Initialize the DatasetManager with search functionality enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be248cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing indices to disk...\n",
      "Time elapsed: 35.987326ms\n",
      "Sorting indices...\n",
      "Time elapsed: 252.669649ms\n",
      "Search functionality initialized successfully!\n",
      "Search handler available: True\n",
      "Search functionality initialized successfully!\n",
      "Search handler available: True\n"
     ]
    }
   ],
   "source": [
    "from tokensmith.manager import DatasetManager\n",
    "\n",
    "dataset_manager = DatasetManager()\n",
    "\n",
    "# Setup search functionality - this builds/loads the search index\n",
    "dataset_manager.setup_search(\n",
    "    bin_file_path=\"../../artifacts/data_ingested_text_document.bin\",\n",
    "    search_index_save_path=\"../../artifacts/search_index_text_document.idx\",\n",
    "    vocab=2**16,  # Use 2**16 for GPT-NeoX tokenizer\n",
    "    verbose=True,\n",
    "    reuse=False,   # Reuse existing index if available\n",
    ")\n",
    "\n",
    "print(\"Search functionality initialized successfully!\")\n",
    "print(f\"Search handler available: {dataset_manager.search is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3da427",
   "metadata": {},
   "source": [
    "## Basic Search Operations\n",
    "\n",
    "Let's start with the fundamental search operations: count, contains, and positions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f9b98b",
   "metadata": {},
   "source": [
    "### Counting Token Sequences\n",
    "\n",
    "The `count()` method tells us how many times a specific token sequence appears in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "382b0d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Phrase Frequency Analysis ===\n",
      "'Once upon a time':\n",
      "  Tokens: [10758, 2220, 247, 673]\n",
      "  Count: 13533\n",
      "\n",
      "'The quick brown fox':\n",
      "  Tokens: [510, 3158, 8516, 30013]\n",
      "  Count: 0\n",
      "\n",
      "'In the beginning':\n",
      "  Tokens: [688, 253, 5068]\n",
      "  Count: 0\n",
      "\n",
      "'It was a dark':\n",
      "  Tokens: [1147, 369, 247, 3644]\n",
      "  Count: 2\n",
      "\n",
      "'Hello world':\n",
      "  Tokens: [12092, 1533]\n",
      "  Count: 0\n",
      "\n",
      "=== Single Token Analysis ===\n",
      "Token 'the' (ID: 783): 12 occurrences\n",
      "Token 'and' (ID: 395): 93 occurrences\n",
      "Token 'to' (ID: 936): 21 occurrences\n",
      "Token 'of' (ID: 1171): 82 occurrences\n",
      "Token 'a' (ID: 66): 510 occurrences\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Search for common phrases\n",
    "common_phrases = [\n",
    "    \"Once upon a time\",\n",
    "    \"The quick brown fox\",\n",
    "    \"In the beginning\",\n",
    "    \"It was a dark\",\n",
    "    \"Hello world\"\n",
    "]\n",
    "\n",
    "print(\"=== Phrase Frequency Analysis ===\")\n",
    "for phrase in common_phrases:\n",
    "    # Convert text to tokens\n",
    "    tokens = tokenizer.encode(phrase, add_special_tokens=False)\n",
    "    count = dataset_manager.search.count(tokens)\n",
    "    \n",
    "    print(f\"'{phrase}':\")\n",
    "    print(f\"  Tokens: {tokens}\")\n",
    "    print(f\"  Count: {count}\")\n",
    "    print()\n",
    "\n",
    "# Example 2: Single token counts\n",
    "print(\"=== Single Token Analysis ===\")\n",
    "common_words = [\"the\", \"and\", \"to\", \"of\", \"a\"]\n",
    "for word in common_words:\n",
    "    token_id = tokenizer.encode(word, add_special_tokens=False)[0]  # Get first token\n",
    "    count = dataset_manager.search.count([token_id])\n",
    "    decoded = tokenizer.decode([token_id])\n",
    "    print(f\"Token '{decoded}' (ID: {token_id}): {count} occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a24b4c7",
   "metadata": {},
   "source": [
    "### Checking Sequence Existence\n",
    "\n",
    "The `contains()` method is useful for quickly checking if a sequence exists without counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "183109d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sequence Existence Check ===\n",
      "✗ Not found: 'machine learning'\n",
      "\n",
      "✗ Not found: 'artificial intelligence'\n",
      "\n",
      "✗ Not found: 'deep neural networks'\n",
      "\n",
      "✗ Not found: 'quantum computing'\n",
      "\n",
      "✗ Not found: 'blockchain technology'\n",
      "\n",
      "✗ Not found: 'this is a very unlikely sequence to exist'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test various sequences for existence\n",
    "test_sequences = [\n",
    "    \"machine learning\",\n",
    "    \"artificial intelligence\", \n",
    "    \"deep neural networks\",\n",
    "    \"quantum computing\",\n",
    "    \"blockchain technology\",\n",
    "    \"this is a very unlikely sequence to exist\"\n",
    "]\n",
    "\n",
    "print(\"=== Sequence Existence Check ===\")\n",
    "for sequence in test_sequences:\n",
    "    tokens = tokenizer.encode(sequence, add_special_tokens=False)\n",
    "    exists = dataset_manager.search.contains(tokens)\n",
    "    status = \"✓ Found\" if exists else \"✗ Not found\"\n",
    "    print(f\"{status}: '{sequence}'\")\n",
    "    \n",
    "    # If found, also get the count\n",
    "    if exists:\n",
    "        count = dataset_manager.search.count(tokens)\n",
    "        print(f\"    Occurrences: {count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5996e4f",
   "metadata": {},
   "source": [
    "### Finding Sequence Positions\n",
    "\n",
    "The `positions()` method returns all locations where a sequence appears in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34b3ba51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Position Analysis for 'Once upon a time' ===\n",
      "Tokens: [10758, 2220, 247, 673]\n",
      "Total occurrences: 13533\n",
      "Positions (first 10): [1871078, 3526259, 739125, 1332842, 1838022, 3434072, 484592, 2798653, 313457, 1297946]\n",
      "\n",
      "Position Statistics:\n",
      "  Average gap: -15.8\n",
      "  Minimum gap: -4493568\n",
      "  Maximum gap: 4541126\n",
      "  First position: 1871078\n",
      "  Last position: 1657484\n"
     ]
    }
   ],
   "source": [
    "# Find positions of specific sequences\n",
    "search_phrase = \"Once upon a time\"\n",
    "tokens = tokenizer.encode(search_phrase, add_special_tokens=False)\n",
    "\n",
    "print(f\"=== Position Analysis for '{search_phrase}' ===\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "positions = dataset_manager.search.positions(tokens)\n",
    "count = len(positions)\n",
    "\n",
    "print(f\"Total occurrences: {count}\")\n",
    "\n",
    "if count > 0:\n",
    "    print(f\"Positions (first 10): {positions[:10]}\")\n",
    "    \n",
    "    # Analyze position distribution\n",
    "    if count > 1:\n",
    "        position_gaps = [positions[i+1] - positions[i] for i in range(len(positions)-1)]\n",
    "        avg_gap = np.mean(position_gaps)\n",
    "        min_gap = min(position_gaps)\n",
    "        max_gap = max(position_gaps)\n",
    "        \n",
    "        print(f\"\\nPosition Statistics:\")\n",
    "        print(f\"  Average gap: {avg_gap:.1f}\")\n",
    "        print(f\"  Minimum gap: {min_gap}\")\n",
    "        print(f\"  Maximum gap: {max_gap}\")\n",
    "        print(f\"  First position: {positions[0]}\")\n",
    "        print(f\"  Last position: {positions[-1]}\")\n",
    "else:\n",
    "    print(\"Sequence not found in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c973c279",
   "metadata": {},
   "source": [
    "## Advanced Search: Next Token Prediction\n",
    "\n",
    "One of the most powerful features is `count_next()`, which shows what tokens typically follow a given sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a634e03",
   "metadata": {},
   "source": [
    "### Analyzing Token Transitions\n",
    "\n",
    "Let's see what commonly follows specific phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52c0f17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Next Token Analysis ===\n",
      "\n",
      "Phrase: 'The cat'\n",
      "Phrase occurs 5 times\n",
      "Followed by 3 different tokens\n",
      "Top continuations:\n",
      "  1. ' is' (ID: 310) - 3 times (60.0%)\n",
      "  2. ' will' (ID: 588) - 1 times (20.0%)\n",
      "  3. ' kept' (ID: 4934) - 1 times (20.0%)\n",
      "\n",
      "Phrase: 'I am'\n",
      "Phrase occurs 392 times\n",
      "Followed by 120 different tokens\n",
      "Top continuations:\n",
      "  1. ' sorry' (ID: 7016) - 74 times (18.9%)\n",
      "  2. ' a' (ID: 247) - 38 times (9.7%)\n",
      "  3. ' the' (ID: 253) - 26 times (6.6%)\n",
      "  4. ' going' (ID: 1469) - 20 times (5.1%)\n",
      "  5. ' so' (ID: 594) - 16 times (4.1%)\n",
      "\n",
      "Phrase: 'Once upon a'\n",
      "Phrase occurs 13542 times\n",
      "Followed by 9 different tokens\n",
      "Top continuations:\n",
      "  1. ' time' (ID: 673) - 13533 times (99.9%)\n",
      "  2. ' Tuesday' (ID: 7948) - 2 times (0.0%)\n",
      "  3. ' long' (ID: 1048) - 1 times (0.0%)\n",
      "  4. ' day' (ID: 1388) - 1 times (0.0%)\n",
      "  5. ' night' (ID: 2360) - 1 times (0.0%)\n",
      "\n",
      "Phrase: 'I am'\n",
      "Phrase occurs 392 times\n",
      "Followed by 120 different tokens\n",
      "Top continuations:\n",
      "  1. ' sorry' (ID: 7016) - 74 times (18.9%)\n",
      "  2. ' a' (ID: 247) - 38 times (9.7%)\n",
      "  3. ' the' (ID: 253) - 26 times (6.6%)\n",
      "  4. ' going' (ID: 1469) - 20 times (5.1%)\n",
      "  5. ' so' (ID: 594) - 16 times (4.1%)\n",
      "\n",
      "Phrase: 'Once upon a'\n",
      "Phrase occurs 13542 times\n",
      "Followed by 9 different tokens\n",
      "Top continuations:\n",
      "  1. ' time' (ID: 673) - 13533 times (99.9%)\n",
      "  2. ' Tuesday' (ID: 7948) - 2 times (0.0%)\n",
      "  3. ' long' (ID: 1048) - 1 times (0.0%)\n",
      "  4. ' day' (ID: 1388) - 1 times (0.0%)\n",
      "  5. ' night' (ID: 2360) - 1 times (0.0%)\n",
      "\n",
      "Phrase: 'In the'\n",
      "Phrase occurs 10 times\n",
      "Followed by 9 different tokens\n",
      "Top continuations:\n",
      "  1. ' park' (ID: 5603) - 2 times (20.0%)\n",
      "  2. ' big' (ID: 1943) - 1 times (10.0%)\n",
      "  3. ' dark' (ID: 3644) - 1 times (10.0%)\n",
      "  4. ' morning' (ID: 4131) - 1 times (10.0%)\n",
      "  5. ' middle' (ID: 4766) - 1 times (10.0%)\n",
      "\n",
      "Phrase: 'She said'\n",
      "Phrase occurs 1 times\n",
      "Followed by 1 different tokens\n",
      "Top continuations:\n",
      "  1. ' it' (ID: 352) - 1 times (100.0%)\n",
      "\n",
      "Phrase: 'In the'\n",
      "Phrase occurs 10 times\n",
      "Followed by 9 different tokens\n",
      "Top continuations:\n",
      "  1. ' park' (ID: 5603) - 2 times (20.0%)\n",
      "  2. ' big' (ID: 1943) - 1 times (10.0%)\n",
      "  3. ' dark' (ID: 3644) - 1 times (10.0%)\n",
      "  4. ' morning' (ID: 4131) - 1 times (10.0%)\n",
      "  5. ' middle' (ID: 4766) - 1 times (10.0%)\n",
      "\n",
      "Phrase: 'She said'\n",
      "Phrase occurs 1 times\n",
      "Followed by 1 different tokens\n",
      "Top continuations:\n",
      "  1. ' it' (ID: 352) - 1 times (100.0%)\n"
     ]
    }
   ],
   "source": [
    "def analyze_next_tokens(phrase: str, top_k: int = 10) -> Dict:\n",
    "    \"\"\"Analyze what tokens commonly follow a given phrase.\"\"\"\n",
    "    tokens = tokenizer.encode(phrase, add_special_tokens=False)\n",
    "    \n",
    "    # Get next token counts\n",
    "    next_counts = dataset_manager.search.count_next(tokens)\n",
    "    \n",
    "    # Find non-zero counts\n",
    "    results = []\n",
    "    for token_id, count in enumerate(next_counts):\n",
    "        if count > 0:\n",
    "            try:\n",
    "                token_text = tokenizer.decode([token_id])\n",
    "                results.append((token_id, token_text, count))\n",
    "            except:\n",
    "                results.append((token_id, f\"[ID:{token_id}]\", count))\n",
    "    \n",
    "    # Sort by count and return top k\n",
    "    results.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    return {\n",
    "        'phrase': phrase,\n",
    "        'phrase_tokens': tokens,\n",
    "        'total_phrase_count': dataset_manager.search.count(tokens),\n",
    "        'top_next_tokens': results[:top_k],\n",
    "        'unique_next_tokens': len(results)\n",
    "    }\n",
    "\n",
    "# Analyze several interesting phrases\n",
    "analysis_phrases = [\n",
    "    \"The cat\",\n",
    "    \"I am\",\n",
    "    \"Once upon a\",\n",
    "    \"In the\",\n",
    "    \"She said\"\n",
    "]\n",
    "\n",
    "print(\"=== Next Token Analysis ===\")\n",
    "for phrase in analysis_phrases:\n",
    "    analysis = analyze_next_tokens(phrase, top_k=5)\n",
    "    \n",
    "    print(f\"\\nPhrase: '{phrase}'\")\n",
    "    print(f\"Phrase occurs {analysis['total_phrase_count']} times\")\n",
    "    print(f\"Followed by {analysis['unique_next_tokens']} different tokens\")\n",
    "    print(\"Top continuations:\")\n",
    "    \n",
    "    for i, (token_id, token_text, count) in enumerate(analysis['top_next_tokens'], 1):\n",
    "        percentage = (count / analysis['total_phrase_count']) * 100\n",
    "        print(f\"  {i}. '{token_text}' (ID: {token_id}) - {count} times ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdf6ede",
   "metadata": {},
   "source": [
    "### Story Continuation Analysis\n",
    "\n",
    "Let's do a deeper analysis of story beginnings to understand narrative patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf68d522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Story Continuation Patterns ===\n",
      "\n",
      "'Once upon a time' (appears 13533 times):\n",
      "  Most common continuations:\n",
      "    1. 'Once upon a time,' (78.7%)\n",
      "    2. 'Once upon a time there' (21.0%)\n",
      "    3. 'Once upon a time in' (0.1%)\n",
      "    4. 'Once upon a time a' (0.1%)\n",
      "    5. 'Once upon a time two' (0.0%)\n",
      "    6. 'Once upon a time on' (0.0%)\n",
      "    7. 'Once upon a time three' (0.0%)\n",
      "\n",
      "'It was a dark and stormy': Not found in dataset\n",
      "\n",
      "'In a land far': Not found in dataset\n",
      "\n",
      "'Long ago': Not found in dataset\n",
      "\n",
      "'There once was' (appears 5 times):\n",
      "  Most common continuations:\n",
      "    1. 'There once was a' (100.0%)\n"
     ]
    }
   ],
   "source": [
    "def story_continuation_analysis():\n",
    "    \"\"\"Analyze how stories typically continue after common openings.\"\"\"\n",
    "    \n",
    "    story_openings = [\n",
    "        \"Once upon a time\",\n",
    "        \"It was a dark and stormy\",\n",
    "        \"In a land far\",\n",
    "        \"Long ago\",\n",
    "        \"There once was\"\n",
    "    ]\n",
    "    \n",
    "    print(\"=== Story Continuation Patterns ===\")\n",
    "    \n",
    "    for opening in story_openings:\n",
    "        tokens = tokenizer.encode(opening, add_special_tokens=False)\n",
    "        phrase_count = dataset_manager.search.count(tokens)\n",
    "        \n",
    "        if phrase_count == 0:\n",
    "            print(f\"\\n'{opening}': Not found in dataset\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n'{opening}' (appears {phrase_count} times):\")\n",
    "        \n",
    "        # Get next tokens\n",
    "        next_counts = dataset_manager.search.count_next(tokens)\n",
    "        \n",
    "        # Build continuations by looking at multiple next tokens\n",
    "        next_tokens = []\n",
    "        for token_id, count in enumerate(next_counts):\n",
    "            if count > 0:\n",
    "                try:\n",
    "                    token_text = tokenizer.decode([token_id])\n",
    "                    next_tokens.append((token_id, token_text, count))\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        # Sort and show top continuations\n",
    "        next_tokens.sort(key=lambda x: x[2], reverse=True)\n",
    "        \n",
    "        print(\"  Most common continuations:\")\n",
    "        for i, (token_id, token_text, count) in enumerate(next_tokens[:7], 1):\n",
    "            # Create continuation phrase\n",
    "            continuation_tokens = tokens + [token_id]\n",
    "            full_continuation = tokenizer.decode(continuation_tokens)\n",
    "            probability = (count / phrase_count) * 100\n",
    "            print(f\"    {i}. '{full_continuation}' ({probability:.1f}%)\")\n",
    "\n",
    "story_continuation_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950256d7",
   "metadata": {},
   "source": [
    "## Batch Search Operations\n",
    "\n",
    "For efficiency when searching multiple sequences, use batch operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c22a7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Batch Next Token Analysis ===\n",
      "Analyzing 5 phrases simultaneously...\n",
      "\n",
      "1. 'The dog' (occurs 22 times):\n",
      "    1. ' is' - 13 times (59.1%)\n",
      "    2. ' was' - 2 times (9.1%)\n",
      "    3. ' might' - 2 times (9.1%)\n",
      "\n",
      "2. 'The cat' (occurs 5 times):\n",
      "    1. ' is' - 3 times (60.0%)\n",
      "    2. ' will' - 1 times (20.0%)\n",
      "    3. ' kept' - 1 times (20.0%)\n",
      "\n",
      "3. 'The bird' (occurs 10 times):\n",
      "    1. ' is' - 6 times (60.0%)\n",
      "    2. ' does' - 2 times (20.0%)\n",
      "    3. 'ie' - 1 times (10.0%)\n",
      "\n",
      "4. 'The fish' (occurs 1 times):\n",
      "    1. ' smiled' - 1 times (100.0%)\n",
      "\n",
      "5. 'The horse' (occurs 2 times):\n",
      "    1. ' is' - 1 times (50.0%)\n",
      "    2. ' feels' - 1 times (50.0%)\n",
      "\n",
      "1. 'The dog' (occurs 22 times):\n",
      "    1. ' is' - 13 times (59.1%)\n",
      "    2. ' was' - 2 times (9.1%)\n",
      "    3. ' might' - 2 times (9.1%)\n",
      "\n",
      "2. 'The cat' (occurs 5 times):\n",
      "    1. ' is' - 3 times (60.0%)\n",
      "    2. ' will' - 1 times (20.0%)\n",
      "    3. ' kept' - 1 times (20.0%)\n",
      "\n",
      "3. 'The bird' (occurs 10 times):\n",
      "    1. ' is' - 6 times (60.0%)\n",
      "    2. ' does' - 2 times (20.0%)\n",
      "    3. 'ie' - 1 times (10.0%)\n",
      "\n",
      "4. 'The fish' (occurs 1 times):\n",
      "    1. ' smiled' - 1 times (100.0%)\n",
      "\n",
      "5. 'The horse' (occurs 2 times):\n",
      "    1. ' is' - 1 times (50.0%)\n",
      "    2. ' feels' - 1 times (50.0%)\n"
     ]
    }
   ],
   "source": [
    "# Batch next token analysis\n",
    "def batch_next_token_analysis():\n",
    "    \"\"\"Demonstrate batch search operations for efficiency.\"\"\"\n",
    "    \n",
    "    # Prepare multiple queries\n",
    "    phrases = [\n",
    "        \"The dog\",\n",
    "        \"The cat\", \n",
    "        \"The bird\",\n",
    "        \"The fish\",\n",
    "        \"The horse\"\n",
    "    ]\n",
    "    \n",
    "    # Convert all phrases to token sequences\n",
    "    token_queries = []\n",
    "    for phrase in phrases:\n",
    "        tokens = tokenizer.encode(phrase, add_special_tokens=False)\n",
    "        token_queries.append(tokens)\n",
    "    \n",
    "    print(\"=== Batch Next Token Analysis ===\")\n",
    "    print(f\"Analyzing {len(phrases)} phrases simultaneously...\")\n",
    "    \n",
    "    # Perform batch search\n",
    "    batch_results = dataset_manager.search.batch_count_next(token_queries)\n",
    "    \n",
    "    # Analyze results\n",
    "    for i, (phrase, query_tokens, next_counts) in enumerate(zip(phrases, token_queries, batch_results)):\n",
    "        phrase_count = dataset_manager.search.count(query_tokens)\n",
    "        \n",
    "        print(f\"\\n{i+1}. '{phrase}' (occurs {phrase_count} times):\")\n",
    "        \n",
    "        # Find top next tokens\n",
    "        next_tokens = []\n",
    "        for token_id, count in enumerate(next_counts):\n",
    "            if count > 0:\n",
    "                try:\n",
    "                    token_text = tokenizer.decode([token_id])\n",
    "                    next_tokens.append((token_text, count))\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        # Sort and display top 3\n",
    "        next_tokens.sort(key=lambda x: x[1], reverse=True)\n",
    "        for j, (token_text, count) in enumerate(next_tokens[:3], 1):\n",
    "            probability = (count / phrase_count) * 100 if phrase_count > 0 else 0\n",
    "            print(f\"    {j}. '{token_text}' - {count} times ({probability:.1f}%)\")\n",
    "\n",
    "batch_next_token_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5d6856",
   "metadata": {},
   "source": [
    "## N-gram Sampling with Smoothing\n",
    "\n",
    "TokenSmith includes advanced n-gram sampling with Kneser-Ney smoothing for generating realistic continuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39f1806b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== N-gram Sampling with Smoothing ===\n",
      "Seed phrase: 'Once upon a time there was a'\n",
      "Seed tokens: [10758, 2220, 247, 673, 627, 369, 247]\n",
      "\n",
      "--- 2-gram Sampling ---\n",
      "Generated 3 continuations:\n",
      "  1. Continuation: 'Once upon a time there was a cork from the microscope from the animals. After'\n",
      "     Full text: 'Once upon a time there was aOnce upon a time there was a cork from the microscope from the animals. After'\n",
      "\n",
      "  2. Continuation: 'Once upon a time there was a great thing they had made a large it. I'\n",
      "     Full text: 'Once upon a time there was aOnce upon a time there was a great thing they had made a large it. I'\n",
      "\n",
      "  3. Continuation: 'Once upon a time there was a big adventure, stick again. He saw a time'\n",
      "     Full text: 'Once upon a time there was aOnce upon a time there was a big adventure, stick again. He saw a time'\n",
      "\n",
      "\n",
      "--- 3-gram Sampling ---\n",
      "Generated 3 continuations:\n",
      "  1. Continuation: 'Once upon a time there was a cork from the microscope from the animals. After'\n",
      "     Full text: 'Once upon a time there was aOnce upon a time there was a cork from the microscope from the animals. After'\n",
      "\n",
      "  2. Continuation: 'Once upon a time there was a great thing they had made a large it. I'\n",
      "     Full text: 'Once upon a time there was aOnce upon a time there was a great thing they had made a large it. I'\n",
      "\n",
      "  3. Continuation: 'Once upon a time there was a big adventure, stick again. He saw a time'\n",
      "     Full text: 'Once upon a time there was aOnce upon a time there was a big adventure, stick again. He saw a time'\n",
      "\n",
      "\n",
      "--- 3-gram Sampling ---\n",
      "Generated 3 continuations:\n",
      "  1. Continuation: 'Once upon a time there was a girl named Lily. She does not know. During'\n",
      "     Full text: 'Once upon a time there was aOnce upon a time there was a girl named Lily. She does not know. During'\n",
      "\n",
      "  2. Continuation: 'Once upon a time there was a little girl named Sally, \"It is too strong'\n",
      "     Full text: 'Once upon a time there was aOnce upon a time there was a little girl named Sally, \"It is too strong'\n",
      "\n",
      "  3. Continuation: 'Once upon a time there was a little bird. \"No, this is my friend'\n",
      "     Full text: 'Once upon a time there was aOnce upon a time there was a little bird. \"No, this is my friend'\n",
      "\n",
      "\n",
      "--- 4-gram Sampling ---\n",
      "Generated 3 continuations:\n",
      "  1. Continuation: 'Once upon a time there was a woman who lived in a small house near the woods'\n",
      "     Full text: 'Once upon a time there was aOnce upon a time there was a woman who lived in a small house near the woods'\n",
      "\n",
      "  2. Continuation: 'Once upon a time there was a little girl, and he would often make sure they'\n",
      "     Full text: 'Once upon a time there was aOnce upon a time there was a little girl, and he would often make sure they'\n",
      "\n",
      "  3. Continuation: 'Once upon a time there was a fish named Fin. Fin loved to swim, fish'\n",
      "     Full text: 'Once upon a time there was aOnce upon a time there was a fish named Fin. Fin loved to swim, fish'\n",
      "\n",
      "Generated 3 continuations:\n",
      "  1. Continuation: 'Once upon a time there was a girl named Lily. She does not know. During'\n",
      "     Full text: 'Once upon a time there was aOnce upon a time there was a girl named Lily. She does not know. During'\n",
      "\n",
      "  2. Continuation: 'Once upon a time there was a little girl named Sally, \"It is too strong'\n",
      "     Full text: 'Once upon a time there was aOnce upon a time there was a little girl named Sally, \"It is too strong'\n",
      "\n",
      "  3. Continuation: 'Once upon a time there was a little bird. \"No, this is my friend'\n",
      "     Full text: 'Once upon a time there was aOnce upon a time there was a little bird. \"No, this is my friend'\n",
      "\n",
      "\n",
      "--- 4-gram Sampling ---\n",
      "Generated 3 continuations:\n",
      "  1. Continuation: 'Once upon a time there was a woman who lived in a small house near the woods'\n",
      "     Full text: 'Once upon a time there was aOnce upon a time there was a woman who lived in a small house near the woods'\n",
      "\n",
      "  2. Continuation: 'Once upon a time there was a little girl, and he would often make sure they'\n",
      "     Full text: 'Once upon a time there was aOnce upon a time there was a little girl, and he would often make sure they'\n",
      "\n",
      "  3. Continuation: 'Once upon a time there was a fish named Fin. Fin loved to swim, fish'\n",
      "     Full text: 'Once upon a time there was aOnce upon a time there was a fish named Fin. Fin loved to swim, fish'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_ngram_sampling():\n",
    "    \"\"\"Demonstrate n-gram sampling with smoothing.\"\"\"\n",
    "    \n",
    "    # Start with a story beginning\n",
    "    seed_phrase = \"Once upon a time there was a\"\n",
    "    seed_tokens = tokenizer.encode(seed_phrase, add_special_tokens=False)\n",
    "    \n",
    "    print(\"=== N-gram Sampling with Smoothing ===\")\n",
    "    print(f\"Seed phrase: '{seed_phrase}'\")\n",
    "    print(f\"Seed tokens: {seed_tokens}\")\n",
    "    \n",
    "    # Generate several continuations using different n-gram orders\n",
    "    n_values = [2, 3, 4]  # bi-gram, tri-gram, 4-gram\n",
    "    \n",
    "    for n in n_values:\n",
    "        print(f\"\\n--- {n}-gram Sampling ---\")\n",
    "        \n",
    "        try:\n",
    "            # Sample continuations\n",
    "            samples = dataset_manager.search.sample_smoothed(\n",
    "                query=seed_tokens,\n",
    "                n=n,           # n-gram order\n",
    "                k=10,          # length of continuation\n",
    "                num_samples=3  # number of samples\n",
    "            )\n",
    "            \n",
    "            print(f\"Generated {len(samples)} continuations:\")\n",
    "            \n",
    "            for i, sample_tokens in enumerate(samples, 1):\n",
    "                # Combine seed and sample\n",
    "                full_sequence = seed_tokens + sample_tokens\n",
    "                full_text = tokenizer.decode(full_sequence)\n",
    "                continuation_text = tokenizer.decode(sample_tokens)\n",
    "                \n",
    "                print(f\"  {i}. Continuation: '{continuation_text}'\")\n",
    "                print(f\"     Full text: '{full_text}'\")\n",
    "                print()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {n}-gram sampling: {e}\")\n",
    "            continue\n",
    "\n",
    "demonstrate_ngram_sampling()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800b03ff",
   "metadata": {},
   "source": [
    "## Real-World Search Applications\n",
    "\n",
    "Let's explore practical applications of search functionality for dataset analysis and research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593f6cdf",
   "metadata": {},
   "source": [
    "### Content Analysis and Filtering\n",
    "\n",
    "Use search to understand the content distribution in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48a212e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset Content Analysis ===\n",
      "\n",
      "Science (Total mentions: 0):\n",
      "  'science': 0\n",
      "  'research': 0\n",
      "  'experiment': 0\n",
      "  'hypothesis': 0\n",
      "  'data': 0\n",
      "\n",
      "Technology (Total mentions: 0):\n",
      "  'computer': 0\n",
      "  'software': 0\n",
      "  'algorithm': 0\n",
      "  'programming': 0\n",
      "  'digital': 0\n",
      "\n",
      "Literature (Total mentions: 1):\n",
      "  'story': 1\n",
      "  'novel': 0\n",
      "  'character': 0\n",
      "  'plot': 0\n",
      "  'narrative': 0\n",
      "\n",
      "Education (Total mentions: 0):\n",
      "  'learn': 0\n",
      "  'teach': 0\n",
      "  'student': 0\n",
      "  'school': 0\n",
      "  'education': 0\n",
      "\n",
      "History (Total mentions: 3):\n",
      "  'war': 3\n",
      "  'history': 0\n",
      "  'ancient': 0\n",
      "  'empire': 0\n",
      "  'civilization': 0\n",
      "\n",
      "Dominant content category: History\n"
     ]
    }
   ],
   "source": [
    "def content_analysis():\n",
    "    \"\"\"Analyze dataset content using search functionality.\"\"\"\n",
    "    \n",
    "    # Define content categories to search for\n",
    "    categories = {\n",
    "        'Science': ['science', 'research', 'experiment', 'hypothesis', 'data'],\n",
    "        'Technology': ['computer', 'software', 'algorithm', 'programming', 'digital'],\n",
    "        'Literature': ['novel', 'story', 'character', 'plot', 'narrative'],\n",
    "        'Education': ['learn', 'teach', 'student', 'school', 'education'],\n",
    "        'History': ['history', 'ancient', 'war', 'empire', 'civilization']\n",
    "    }\n",
    "    \n",
    "    print(\"=== Dataset Content Analysis ===\")\n",
    "    \n",
    "    category_scores = {}\n",
    "    \n",
    "    for category, keywords in categories.items():\n",
    "        total_score = 0\n",
    "        keyword_results = []\n",
    "        \n",
    "        for keyword in keywords:\n",
    "            tokens = tokenizer.encode(keyword, add_special_tokens=False)\n",
    "            count = dataset_manager.search.count(tokens)\n",
    "            total_score += count\n",
    "            keyword_results.append((keyword, count))\n",
    "        \n",
    "        category_scores[category] = {\n",
    "            'total_score': total_score,\n",
    "            'keywords': keyword_results\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{category} (Total mentions: {total_score}):\")\n",
    "        # Sort keywords by frequency\n",
    "        keyword_results.sort(key=lambda x: x[1], reverse=True)\n",
    "        for keyword, count in keyword_results:\n",
    "            print(f\"  '{keyword}': {count}\")\n",
    "    \n",
    "    # Find dominant category\n",
    "    dominant_category = max(category_scores.keys(), key=lambda k: category_scores[k]['total_score'])\n",
    "    print(f\"\\nDominant content category: {dominant_category}\")\n",
    "    \n",
    "    return category_scores\n",
    "\n",
    "content_scores = content_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936c0d0f",
   "metadata": {},
   "source": [
    "### Quality Assessment\n",
    "\n",
    "Use search to identify potential quality issues in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71b7ccc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset Quality Assessment ===\n",
      "\n",
      "1. Repetitive Pattern Detection:\n",
      "  ✓ No obvious repetitive patterns found\n",
      "\n",
      "2. Potential Encoding Issues:\n",
      "  ✓ No obvious encoding issues found\n",
      "\n",
      "3. Placeholder Text Detection:\n",
      "  ✓ No placeholder text found\n"
     ]
    }
   ],
   "source": [
    "def quality_assessment():\n",
    "    \"\"\"Use search to assess dataset quality.\"\"\"\n",
    "    \n",
    "    print(\"=== Dataset Quality Assessment ===\")\n",
    "    \n",
    "    # Check for repetitive patterns\n",
    "    repetitive_patterns = [\n",
    "        \"the the\",\n",
    "        \"and and\", \n",
    "        \"is is\",\n",
    "        \"to to\",\n",
    "        \"a a a\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n1. Repetitive Pattern Detection:\")\n",
    "    repetitive_found = False\n",
    "    for pattern in repetitive_patterns:\n",
    "        tokens = tokenizer.encode(pattern, add_special_tokens=False)\n",
    "        count = dataset_manager.search.count(tokens)\n",
    "        if count > 0:\n",
    "            print(f\"  '{pattern}': {count} occurrences ⚠️\")\n",
    "            repetitive_found = True\n",
    "    \n",
    "    if not repetitive_found:\n",
    "        print(\"  ✓ No obvious repetitive patterns found\")\n",
    "    \n",
    "    # Check for encoding issues\n",
    "    print(\"\\n2. Potential Encoding Issues:\")\n",
    "    encoding_issues = [\n",
    "        \"\\\\n\",  # Escaped newlines\n",
    "        \"\\\\t\",  # Escaped tabs\n",
    "        \"\\\\r\",  # Escaped carriage returns\n",
    "        \"â€™\", # Common encoding artifact\n",
    "        \"â€œ\", # Another common artifact\n",
    "    ]\n",
    "    \n",
    "    encoding_found = False\n",
    "    for issue in encoding_issues:\n",
    "        tokens = tokenizer.encode(issue, add_special_tokens=False)\n",
    "        count = dataset_manager.search.count(tokens)\n",
    "        if count > 0:\n",
    "            print(f\"  '{issue}': {count} occurrences ⚠️\")\n",
    "            encoding_found = True\n",
    "    \n",
    "    if not encoding_found:\n",
    "        print(\"  ✓ No obvious encoding issues found\")\n",
    "    \n",
    "    # Check for placeholder text\n",
    "    print(\"\\n3. Placeholder Text Detection:\")\n",
    "    placeholders = [\n",
    "        \"lorem ipsum\",\n",
    "        \"placeholder text\",\n",
    "        \"sample text\",\n",
    "        \"TODO\",\n",
    "        \"FIXME\"\n",
    "    ]\n",
    "    \n",
    "    placeholder_found = False\n",
    "    for placeholder in placeholders:\n",
    "        tokens = tokenizer.encode(placeholder, add_special_tokens=False)\n",
    "        count = dataset_manager.search.count(tokens)\n",
    "        if count > 0:\n",
    "            print(f\"  '{placeholder}': {count} occurrences ⚠️\")\n",
    "            placeholder_found = True\n",
    "    \n",
    "    if not placeholder_found:\n",
    "        print(\"  ✓ No placeholder text found\")\n",
    "\n",
    "quality_assessment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16b8e42",
   "metadata": {},
   "source": [
    "### Language Pattern Analysis\n",
    "\n",
    "Analyze linguistic patterns and style in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66bffca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Linguistic Pattern Analysis ===\n",
      "\n",
      "1. Common Sentence Starters:\n",
      "   1. 'I': 6164\n",
      "   2. 'It': 2266\n",
      "   3. 'We': 1426\n",
      "   4. 'The': 511\n",
      "   5. 'A': 182\n",
      "   6. 'They': 136\n",
      "   7. 'He': 121\n",
      "   8. 'She': 52\n",
      "   9. 'An': 24\n",
      "  10. 'In': 16\n",
      "\n",
      "2. Question Patterns:\n",
      "  'What': 1863\n",
      "  'Where': 240\n",
      "  'When': 24\n",
      "  'Why': 561\n",
      "  'How': 151\n",
      "  'Who': 201\n",
      "  'Which': 13\n",
      "  Total question indicators: 3053\n",
      "\n",
      "3. Temporal Indicators:\n",
      "  'today': 1\n",
      "  'now': 7\n",
      "  'then': 1\n"
     ]
    }
   ],
   "source": [
    "def linguistic_analysis():\n",
    "    \"\"\"Analyze linguistic patterns in the dataset.\"\"\"\n",
    "    \n",
    "    print(\"=== Linguistic Pattern Analysis ===\")\n",
    "    \n",
    "    # Analyze sentence starters\n",
    "    print(\"\\n1. Common Sentence Starters:\")\n",
    "    sentence_starters = [\n",
    "        \"The\", \"A\", \"An\", \"I\", \"We\", \"They\", \"He\", \"She\", \"It\",\n",
    "        \"In\", \"On\", \"At\", \"With\", \"For\", \"During\", \"After\", \"Before\"\n",
    "    ]\n",
    "    \n",
    "    starter_counts = []\n",
    "    for starter in sentence_starters:\n",
    "        tokens = tokenizer.encode(starter, add_special_tokens=False)\n",
    "        count = dataset_manager.search.count(tokens)\n",
    "        starter_counts.append((starter, count))\n",
    "    \n",
    "    # Sort by frequency\n",
    "    starter_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "    for i, (starter, count) in enumerate(starter_counts[:10], 1):\n",
    "        print(f\"  {i:2d}. '{starter}': {count}\")\n",
    "    \n",
    "    # Analyze question patterns\n",
    "    print(\"\\n2. Question Patterns:\")\n",
    "    question_words = [\"What\", \"Where\", \"When\", \"Why\", \"How\", \"Who\", \"Which\"]\n",
    "    \n",
    "    total_questions = 0\n",
    "    for word in question_words:\n",
    "        tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "        count = dataset_manager.search.count(tokens)\n",
    "        total_questions += count\n",
    "        print(f\"  '{word}': {count}\")\n",
    "    \n",
    "    print(f\"  Total question indicators: {total_questions}\")\n",
    "    \n",
    "    # Analyze temporal indicators\n",
    "    print(\"\\n3. Temporal Indicators:\")\n",
    "    temporal_words = [\n",
    "        \"yesterday\", \"today\", \"tomorrow\", \n",
    "        \"now\", \"then\", \"later\", \"soon\",\n",
    "        \"before\", \"after\", \"during\", \"while\"\n",
    "    ]\n",
    "    \n",
    "    for word in temporal_words:\n",
    "        tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "        count = dataset_manager.search.count(tokens)\n",
    "        if count > 0:\n",
    "            print(f\"  '{word}': {count}\")\n",
    "\n",
    "linguistic_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4997ec25",
   "metadata": {},
   "source": [
    "## Search Performance and Optimization\n",
    "\n",
    "Understanding search performance helps optimize your analysis workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a5065d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Search Performance Analysis ===\n",
      "\n",
      "1. Query Length Performance:\n",
      "  Single token ('the'):\n",
      "    Count: 12 (Time: 0.0000s)\n",
      "    Positions: 0.0000s\n",
      "  Two tokens ('the quick'):\n",
      "    Count: 0 (Time: 0.0000s)\n",
      "    Positions: 0.0000s\n",
      "  Four tokens ('the quick brown fox'):\n",
      "    Count: 0 (Time: 0.0000s)\n",
      "    Positions: 0.0000s\n",
      "  Seven tokens ('the quick brown fox jumps over the'):\n",
      "    Count: 0 (Time: 0.0000s)\n",
      "    Positions: 0.0000s\n",
      "\n",
      "2. Batch vs Individual Operations:\n",
      "  Individual operations: 0.4445s\n",
      "  Batch operation: 0.9893s\n",
      "  Speedup: 0.45x\n",
      "\n",
      "3. Query Frequency Impact:\n",
      "  Common query ('the'): 12 results in 0.0001s\n",
      "  Rare query ('the quick brown fox jumps over'): 0 results in 0.0000s\n",
      "  Individual operations: 0.4445s\n",
      "  Batch operation: 0.9893s\n",
      "  Speedup: 0.45x\n",
      "\n",
      "3. Query Frequency Impact:\n",
      "  Common query ('the'): 12 results in 0.0001s\n",
      "  Rare query ('the quick brown fox jumps over'): 0 results in 0.0000s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def search_performance_analysis():\n",
    "    \"\"\"Analyze search performance for different query types.\"\"\"\n",
    "    \n",
    "    print(\"=== Search Performance Analysis ===\")\n",
    "    \n",
    "    # Pre-tokenize a long sequence for consistent testing\n",
    "    test_sequence = \"the quick brown fox jumps over the lazy dog in the park\"\n",
    "    all_tokens = tokenizer.encode(test_sequence, add_special_tokens=False)\n",
    "    \n",
    "    # Test different query lengths using slices of the same sequence\n",
    "    test_queries = [\n",
    "        (all_tokens[:1], \"Single token\"),\n",
    "        (all_tokens[:2], \"Two tokens\"),\n",
    "        (all_tokens[:4], \"Four tokens\"),\n",
    "        (all_tokens[:7], \"Seven tokens\")\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n1. Query Length Performance:\")\n",
    "    for tokens, description in test_queries:\n",
    "        query_text = tokenizer.decode(tokens)\n",
    "        \n",
    "        # Time the count operation\n",
    "        start_time = time.time()\n",
    "        count = dataset_manager.search.count(tokens)\n",
    "        count_time = time.time() - start_time\n",
    "        \n",
    "        # Time the positions operation (if count is reasonable)\n",
    "        if count < 1000:  # Only get positions for reasonable counts\n",
    "            start_time = time.time()\n",
    "            positions = dataset_manager.search.positions(tokens)\n",
    "            positions_time = time.time() - start_time\n",
    "        else:\n",
    "            positions_time = \"N/A (too many results)\"\n",
    "        \n",
    "        print(f\"  {description} ('{query_text}'):\")\n",
    "        print(f\"    Count: {count} (Time: {count_time:.4f}s)\")\n",
    "        print(f\"    Positions: {positions_time if isinstance(positions_time, str) else f'{positions_time:.4f}s'}\")\n",
    "    \n",
    "    # Test batch vs individual operations\n",
    "    print(\"\\n2. Batch vs Individual Operations:\")\n",
    "    \n",
    "    # Use consistent tokenized queries\n",
    "    base_phrase = \"Once upon a time there was a dog and a cat and a bird\"\n",
    "    base_tokens = tokenizer.encode(base_phrase, add_special_tokens=False)\n",
    "    \n",
    "    batch_queries = [\n",
    "        base_tokens[:2],  # \"the dog\"\n",
    "        base_tokens[4:6], # \"the cat\"\n",
    "        base_tokens[8:10] # \"the bird\"\n",
    "    ]*100\n",
    "    \n",
    "    # Individual operations\n",
    "    start_time = time.time()\n",
    "    individual_results = []\n",
    "    for query in batch_queries:\n",
    "        result = dataset_manager.search.count_next(query)\n",
    "        individual_results.append(result)\n",
    "    individual_time = time.time() - start_time\n",
    "    \n",
    "    # Batch operation\n",
    "    start_time = time.time()\n",
    "    batch_results = dataset_manager.search.batch_count_next(batch_queries)\n",
    "    batch_time = time.time() - start_time\n",
    "\n",
    "    assert len(individual_results) == len(batch_results), \"Batch results length mismatch\"\n",
    "    \n",
    "    print(f\"  Individual operations: {individual_time:.4f}s\")\n",
    "    print(f\"  Batch operation: {batch_time:.4f}s\")\n",
    "    print(f\"  Speedup: {individual_time/batch_time:.2f}x\" if batch_time > 0 else \"  Batch operation too fast to measure accurately\")\n",
    "    \n",
    "    # Test query frequency impact\n",
    "    print(\"\\n3. Query Frequency Impact:\")\n",
    "    \n",
    "    # Test common vs rare sequences\n",
    "    common_tokens = all_tokens[:1]  # Very common single token\n",
    "    rare_tokens = all_tokens[:6]    # Potentially rare 6-token sequence\n",
    "    \n",
    "    # Test common query\n",
    "    start_time = time.time()\n",
    "    common_count = dataset_manager.search.count(common_tokens)\n",
    "    common_time = time.time() - start_time\n",
    "    \n",
    "    # Test rare query  \n",
    "    start_time = time.time()\n",
    "    rare_count = dataset_manager.search.count(rare_tokens)\n",
    "    rare_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"  Common query ('{tokenizer.decode(common_tokens)}'): {common_count} results in {common_time:.4f}s\")\n",
    "    print(f\"  Rare query ('{tokenizer.decode(rare_tokens)}'): {rare_count} results in {rare_time:.4f}s\")\n",
    "\n",
    "search_performance_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37537d32",
   "metadata": {},
   "source": [
    "## Advanced Use Cases\n",
    "\n",
    "Let's explore some advanced use cases that demonstrate the full power of search functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c429b27b",
   "metadata": {},
   "source": [
    "### Building Custom Language Models\n",
    "\n",
    "Use search results to build simple language models or probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f10eae42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Building Simple Language Model ===\n",
      "Context: 'Once'\n",
      "Context tokens: [10758]\n",
      "Context appears 15326 times\n",
      "\n",
      "Top 10 most likely next tokens:\n",
      "   1. ' upon' (ID: 2220)\n",
      "      Probability: 0.8838 (88.4%)\n",
      "      Count: 13545\n",
      "   2. ' there' (ID: 627)\n",
      "      Probability: 0.1088 (10.9%)\n",
      "      Count: 1667\n",
      "   3. ',' (ID: 13)\n",
      "      Probability: 0.0071 (0.7%)\n",
      "      Count: 109\n",
      "   4. ' a' (ID: 247)\n",
      "      Probability: 0.0001 (0.0%)\n",
      "      Count: 1\n",
      "   5. ' in' (ID: 275)\n",
      "      Probability: 0.0001 (0.0%)\n",
      "      Count: 1\n",
      "   6. ' it' (ID: 352)\n",
      "      Probability: 0.0001 (0.0%)\n",
      "      Count: 1\n",
      "   7. ' Mary' (ID: 6393)\n",
      "      Probability: 0.0001 (0.0%)\n",
      "      Count: 1\n",
      "   8. ' Upon' (ID: 15797)\n",
      "      Probability: 0.0001 (0.0%)\n",
      "      Count: 1\n",
      "\n",
      "Top 10 tokens cover 100.0% of all continuations\n",
      "\n",
      "--- Sample Generations ---\n",
      "\n",
      "Generation 1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Full text: 'Once upon a time, there'\n",
      "  Continuation: ' upon a time, there'\n",
      "\n",
      "Generation 2:\n",
      "  Full text: 'Once upon a time, there'\n",
      "  Continuation: ' upon a time, there'\n",
      "\n",
      "Generation 3:\n",
      "  Full text: 'Once upon a time, in'\n",
      "  Continuation: ' upon a time, in'\n"
     ]
    }
   ],
   "source": [
    "def build_simple_language_model():\n",
    "    \"\"\"Build a simple n-gram language model using search results.\"\"\"\n",
    "    \n",
    "    print(\"=== Building Simple Language Model ===\")\n",
    "    \n",
    "    # Pre-tokenize a longer context to ensure we have consistent token sequences\n",
    "    full_context = \"Once\"\n",
    "    full_tokens = tokenizer.encode(full_context, add_special_tokens=False)\n",
    "    \n",
    "    # Use first 2 tokens as our context\n",
    "    context_tokens = full_tokens[:2]\n",
    "    context = tokenizer.decode(context_tokens)\n",
    "    \n",
    "    print(f\"Context: '{context}'\")\n",
    "    print(f\"Context tokens: {context_tokens}\")\n",
    "    \n",
    "    # Get next token distribution\n",
    "    next_counts = dataset_manager.search.count_next(context_tokens)\n",
    "    context_count = dataset_manager.search.count(context_tokens)\n",
    "    \n",
    "    if context_count == 0:\n",
    "        print(\"Context not found in dataset\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Context appears {context_count} times\")\n",
    "    \n",
    "    # Build probability distribution\n",
    "    probabilities = []\n",
    "    for token_id, count in enumerate(next_counts):\n",
    "        if count > 0:\n",
    "            prob = count / context_count\n",
    "            try:\n",
    "                token_text = tokenizer.decode([token_id])\n",
    "                probabilities.append((token_id, token_text, count, prob))\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # Sort by probability\n",
    "    probabilities.sort(key=lambda x: x[3], reverse=True)\n",
    "    \n",
    "    print(f\"\\nTop 10 most likely next tokens:\")\n",
    "    cumulative_prob = 0\n",
    "    for i, (token_id, token_text, count, prob) in enumerate(probabilities[:10], 1):\n",
    "        cumulative_prob += prob\n",
    "        print(f\"  {i:2d}. '{token_text}' (ID: {token_id})\")\n",
    "        print(f\"      Probability: {prob:.4f} ({prob*100:.1f}%)\")\n",
    "        print(f\"      Count: {count}\")\n",
    "    \n",
    "    print(f\"\\nTop 10 tokens cover {cumulative_prob:.1%} of all continuations\")\n",
    "    \n",
    "    # Generate sample text using the model\n",
    "    print(f\"\\n--- Sample Generations ---\")\n",
    "    \n",
    "    for generation in range(3):\n",
    "        print(f\"\\nGeneration {generation + 1}:\")\n",
    "        generation_tokens = context_tokens.copy()\n",
    "        \n",
    "        # Generate 5 more tokens\n",
    "        for step in range(5):\n",
    "            # Use consistent context length (2 tokens)\n",
    "            current_context = generation_tokens[-2:]\n",
    "            next_counts = dataset_manager.search.count_next(current_context)\n",
    "            context_total = sum(next_counts)\n",
    "            \n",
    "            if context_total == 0:\n",
    "                print(f\"    No continuations found for context: {tokenizer.decode(current_context)}\")\n",
    "                break\n",
    "            \n",
    "            # Sample next token based on probability\n",
    "            next_probs = [count / context_total for count in next_counts]\n",
    "            \n",
    "            # Handle case where all probabilities are zero\n",
    "            if sum(next_probs) == 0:\n",
    "                print(f\"    No valid continuations for context: {tokenizer.decode(current_context)}\")\n",
    "                break\n",
    "                \n",
    "            next_token = np.random.choice(len(next_probs), p=next_probs)\n",
    "            generation_tokens.append(next_token)\n",
    "        \n",
    "        generated_text = tokenizer.decode(generation_tokens)\n",
    "        continuation_text = tokenizer.decode(generation_tokens[len(context_tokens):])\n",
    "        print(f\"  Full text: '{generated_text}'\")\n",
    "        print(f\"  Continuation: '{continuation_text}'\")\n",
    "\n",
    "build_simple_language_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f18f42",
   "metadata": {},
   "source": [
    "### Dataset Comparison\n",
    "\n",
    "Compare different datasets or dataset versions using search statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17f6bcb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset Search Signature ===\n",
      "Computing dataset signature...\n",
      "\n",
      "Dataset Signature (Total signature tokens: 8188):\n",
      "  'I':   6164 ( 75.3%)\n",
      "  'a':    510 (  6.2%)\n",
      "  'where':    351 (  4.3%)\n",
      "  'we':    248 (  3.0%)\n",
      "  'an':    210 (  2.6%)\n",
      "  'year':    125 (  1.5%)\n",
      "  'day':    101 (  1.2%)\n",
      "  'and':     93 (  1.1%)\n",
      "  'time':     81 (  1.0%)\n",
      "  'is':     78 (  1.0%)\n",
      "  'was':     67 (  0.8%)\n",
      "  'you':     44 (  0.5%)\n",
      "  'what':     20 (  0.2%)\n",
      "  'or':     16 (  0.2%)\n",
      "  'but':     15 (  0.2%)\n",
      "  'she':     13 (  0.2%)\n",
      "  'the':     12 (  0.1%)\n",
      "  'are':     11 (  0.1%)\n",
      "  'now':      7 (  0.1%)\n",
      "  'he':      4 (  0.0%)\n",
      "  'how':      4 (  0.0%)\n",
      "  'have':      3 (  0.0%)\n",
      "  'why':      3 (  0.0%)\n",
      "  'they':      2 (  0.0%)\n",
      "  'if':      2 (  0.0%)\n",
      "  'when':      1 (  0.0%)\n",
      "  'in the':      1 (  0.0%)\n",
      "  'today':      1 (  0.0%)\n",
      "  'were':      0 (  0.0%)\n",
      "  'has':      0 (  0.0%)\n",
      "  'of the':      0 (  0.0%)\n",
      "  'to the':      0 (  0.0%)\n",
      "  'and the':      0 (  0.0%)\n"
     ]
    }
   ],
   "source": [
    "def create_search_signature():\n",
    "    \"\"\"Create a 'signature' of the dataset using search statistics.\"\"\"\n",
    "    \n",
    "    print(\"=== Dataset Search Signature ===\")\n",
    "    \n",
    "    # Define signature queries - common patterns that characterize text\n",
    "    signature_queries = [\n",
    "        # Articles\n",
    "        \"the\", \"a\", \"an\",\n",
    "        # Pronouns\n",
    "        \"I\", \"you\", \"he\", \"she\", \"we\", \"they\",\n",
    "        # Common verbs\n",
    "        \"is\", \"was\", \"are\", \"were\", \"have\", \"has\",\n",
    "        # Conjunctions\n",
    "        \"and\", \"or\", \"but\", \"if\", \"when\",\n",
    "        # Common phrases\n",
    "        \"of the\", \"in the\", \"to the\", \"and the\",\n",
    "        # Question words\n",
    "        \"what\", \"where\", \"when\", \"why\", \"how\",\n",
    "        # Temporal\n",
    "        \"time\", \"day\", \"year\", \"today\", \"now\"\n",
    "    ]\n",
    "    \n",
    "    signature = {}\n",
    "    total_signature_count = 0\n",
    "    \n",
    "    print(\"Computing dataset signature...\")\n",
    "    \n",
    "    for query_text in signature_queries:\n",
    "        tokens = tokenizer.encode(query_text, add_special_tokens=False)\n",
    "        count = dataset_manager.search.count(tokens)\n",
    "        signature[query_text] = count\n",
    "        total_signature_count += count\n",
    "    \n",
    "    # Normalize to percentages\n",
    "    signature_percentages = {}\n",
    "    for query_text, count in signature.items():\n",
    "        percentage = (count / total_signature_count) * 100 if total_signature_count > 0 else 0\n",
    "        signature_percentages[query_text] = percentage\n",
    "    \n",
    "    print(f\"\\nDataset Signature (Total signature tokens: {total_signature_count}):\")\n",
    "    \n",
    "    # Sort by frequency\n",
    "    sorted_signature = sorted(signature.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for query_text, count in sorted_signature:\n",
    "        percentage = signature_percentages[query_text]\n",
    "        print(f\"  '{query_text}': {count:6d} ({percentage:5.1f}%)\")\n",
    "    \n",
    "    return signature\n",
    "\n",
    "signature = create_search_signature()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b410fce7",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "Let's wrap up with a summary of search functionality and best practices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b08092",
   "metadata": {},
   "source": [
    "## Search Functionality Summary\n",
    "\n",
    "🔍 **CORE SEARCH METHODS:**\n",
    "- `dataset_manager.search.count(query)` → Count occurrences of token sequence\n",
    "- `dataset_manager.search.contains(query)` → Check if sequence exists (faster than count)\n",
    "- `dataset_manager.search.positions(query)` → Get all positions where sequence appears\n",
    "- `dataset_manager.search.count_next(query)` → Count what tokens follow the sequence\n",
    "- `dataset_manager.search.batch_count_next(queries)` → Batch version for multiple queries\n",
    "\n",
    "🧠 **ADVANCED FEATURES:**\n",
    "- `dataset_manager.search.sample_smoothed(query, n, k, num_samples)` → Generate continuations using Kneser-Ney smoothing\n",
    "- Efficient indexing with vocabulary size optimization\n",
    "- Memory-mapped index for large datasets\n",
    "- Support for 2^16 and 2^32 vocabulary sizes\n",
    "\n",
    "💡 **PRACTICAL APPLICATIONS:**\n",
    "- Content analysis and categorization\n",
    "- Dataset quality assessment  \n",
    "- Language pattern analysis\n",
    "- Next token prediction and modeling\n",
    "- Linguistic research and analysis\n",
    "- Dataset comparison and signatures\n",
    "\n",
    "⚡ **PERFORMANCE TIPS:**\n",
    "- Use batch operations for multiple queries\n",
    "- Check exists before getting positions for rare sequences\n",
    "- Use appropriate vocabulary size (2^16 vs 2^32)\n",
    "- Reuse indexes when possible\n",
    "- Consider query length impact on performance\n",
    "\n",
    "🛠️ **BEST PRACTICES:**\n",
    "- Always validate token sequences before search\n",
    "- Handle edge cases (empty results, encoding issues)\n",
    "- Use meaningful variable names for token sequences\n",
    "- Consider memory usage for large result sets\n",
    "- Cache frequently used search results\n",
    "- Combine search with other TokenSmith handlers for powerful workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b525e24",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Congratulations! You've learned how to use TokenSmith's search functionality effectively. Here are some suggested next steps:\n",
    "\n",
    "### 🎯 **Immediate Actions:**\n",
    "1. **Experiment** with your own dataset using the search methods\n",
    "2. **Combine** search with sampling and editing for powerful workflows  \n",
    "3. **Build** custom analysis tools using the search results\n",
    "4. **Optimize** your search queries for better performance\n",
    "\n",
    "### 📚 **Additional Resources:**\n",
    "- **[TokenSmith Documentation](https://aflah02.github.io/tokensmith)** - Complete API reference\n",
    "- **[Basic Setup Tutorial](01_basic_setup.ipynb)** - Getting started with TokenSmith\n",
    "- **[Inspection Tutorial](02_inspect_samples.ipynb)** - Dataset examination techniques  \n",
    "- **[Sampling Tutorial](03_sampling_methods.ipynb)** - Flexible data sampling strategies\n",
    "- **[Editing Tutorial](04_dataset_editing_methods.ipynb)** - Dataset modification techniques\n",
    "\n",
    "### 🚀 **Advanced Projects:**\n",
    "1. **Language Model Analysis**: Use search to analyze and compare different language models\n",
    "2. **Content Classification**: Build automated content classifiers using search patterns\n",
    "3. **Dataset Curation**: Use search to identify and filter high-quality content\n",
    "4. **Linguistic Research**: Investigate language patterns and evolution in large corpora\n",
    "5. **Quality Control**: Build automated quality assessment pipelines\n",
    "\n",
    "### 🔬 **Research Applications:**\n",
    "- **Bias Detection**: Search for potentially biased patterns in training data\n",
    "- **Memorization Studies**: Identify memorized content in language models\n",
    "- **Distribution Analysis**: Understand token and phrase distributions\n",
    "- **Cross-lingual Analysis**: Compare patterns across different languages\n",
    "- **Temporal Analysis**: Track language change over time in timestamped datasets\n",
    "\n",
    "Happy searching! 🔍✨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neox_updated_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
