{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6430cde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eadb2cf",
   "metadata": {},
   "source": [
    "# Download and Prepare Different Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1938745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-02 08:55:48--  https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0/resolve/main/global-shard_01_of_10/local-shard_0_of_10/shard_00000000_processed.jsonl.zst\n",
      "Resolving huggingface.co (huggingface.co)... 52.222.136.89, 52.222.136.38, 52.222.136.117, ...\n",
      "Connecting to huggingface.co (huggingface.co)|52.222.136.89|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs-us-1.hf.co/repos/6d/96/6d960e289759d6a146125cb84c6134b9c0bc4344e4c59e0b3e902698997e5fe7/c006fce0b12ea2366ed94503344c7bc539e17c0aa10159a632272dd6d2929bdb?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27shard_00000000_processed.jsonl.zst%3B+filename%3D%22shard_00000000_processed.jsonl.zst%22%3B&Expires=1751442948&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MTQ0Mjk0OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzZkLzk2LzZkOTYwZTI4OTc1OWQ2YTE0NjEyNWNiODRjNjEzNGI5YzBiYzQzNDRlNGM1OWUwYjNlOTAyNjk4OTk3ZTVmZTcvYzAwNmZjZTBiMTJlYTIzNjZlZDk0NTAzMzQ0YzdiYzUzOWUxN2MwYWExMDE1OWE2MzIyNzJkZDZkMjkyOWJkYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=YsvhnvmJlJQQJ-7P1x9VWg%7EKj9L042M0pc11JTtK9oe2fNe2UuywWsYcRwIN7jpdUU8tgKIY1t1nYFZOxkHDGiQwA61sWL2mistZCMtW%7EjiOoUt3cjfqnhsNx6RgT1dz5hMNXxokFP9SpUWR90dpZtL-bQnhhR9KU4z2Q6Fe0oS4j2t-F0a7VvwnGqvBMJ-DnqeJWmHF-vXkXiDIu-oQclvAx2NwCkLm%7E2NMjLxjuTL2LOcLSHBEY2%7E4AP3jPTRq6C2HVvXnrubNBv3KIKdRqMXYouezh90Bm1mOHxW-xkMhkBR3KkB1h2aNQN7V6FE8N2YzSDSYyBTb5bn8R3%7E9Kg__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
      "--2025-07-02 08:55:48--  https://cdn-lfs-us-1.hf.co/repos/6d/96/6d960e289759d6a146125cb84c6134b9c0bc4344e4c59e0b3e902698997e5fe7/c006fce0b12ea2366ed94503344c7bc539e17c0aa10159a632272dd6d2929bdb?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27shard_00000000_processed.jsonl.zst%3B+filename%3D%22shard_00000000_processed.jsonl.zst%22%3B&Expires=1751442948&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MTQ0Mjk0OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzZkLzk2LzZkOTYwZTI4OTc1OWQ2YTE0NjEyNWNiODRjNjEzNGI5YzBiYzQzNDRlNGM1OWUwYjNlOTAyNjk4OTk3ZTVmZTcvYzAwNmZjZTBiMTJlYTIzNjZlZDk0NTAzMzQ0YzdiYzUzOWUxN2MwYWExMDE1OWE2MzIyNzJkZDZkMjkyOWJkYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=YsvhnvmJlJQQJ-7P1x9VWg%7EKj9L042M0pc11JTtK9oe2fNe2UuywWsYcRwIN7jpdUU8tgKIY1t1nYFZOxkHDGiQwA61sWL2mistZCMtW%7EjiOoUt3cjfqnhsNx6RgT1dz5hMNXxokFP9SpUWR90dpZtL-bQnhhR9KU4z2Q6Fe0oS4j2t-F0a7VvwnGqvBMJ-DnqeJWmHF-vXkXiDIu-oQclvAx2NwCkLm%7E2NMjLxjuTL2LOcLSHBEY2%7E4AP3jPTRq6C2HVvXnrubNBv3KIKdRqMXYouezh90Bm1mOHxW-xkMhkBR3KkB1h2aNQN7V6FE8N2YzSDSYyBTb5bn8R3%7E9Kg__&Key-Pair-Id=K24J24Z295AEI9\n",
      "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 13.32.99.39, 13.32.99.52, 13.32.99.63, ...\n",
      "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|13.32.99.39|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 141306362 (135M) [binary/octet-stream]\n",
      "Saving to: ‘shard_00000000_processed.jsonl.zst.1’\n",
      "\n",
      "shard_00000000_proc 100%[===================>] 134.76M   165MB/s    in 0.8s    \n",
      "\n",
      "2025-07-02 08:55:49 (165 MB/s) - ‘shard_00000000_processed.jsonl.zst.1’ saved [141306362/141306362]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0/resolve/main/global-shard_01_of_10/local-shard_0_of_10/shard_00000000_processed.jsonl.zst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d99baf9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zstd: shard_00000000_processed.jsonl already exists; overwrite (y/n) ? "
     ]
    }
   ],
   "source": [
    "# Extract the downloaded file\n",
    "!zstd -d shard_00000000_processed.jsonl.zst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "840adbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_data_path = 'shard_00000000_processed.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8d6418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(jsonl_data_path, 'r') as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70c9f02a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61a46adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 27988.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File benchmarking_data/data_1.jsonl already exists, skipping...\n",
      "File benchmarking_data/data_10.jsonl already exists, skipping...\n",
      "File benchmarking_data/data_100.jsonl already exists, skipping...\n",
      "File benchmarking_data/data_1000.jsonl already exists, skipping...\n",
      "File benchmarking_data/data_10000.jsonl already exists, skipping...\n",
      "File benchmarking_data/data_100000.jsonl already exists, skipping...\n",
      "File benchmarking_data/data_1000000.jsonl already exists, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create JSONL files of different document sizes, for smaller sizes than len(data) drop remaining documents and for larger sizes pad with copies of the first K documents where K is the remaining number of documents\n",
    "\n",
    "sizes = [1, 10, 100, 1000, 10000, 100000, 1000000]\n",
    "\n",
    "for size in tqdm(sizes):\n",
    "\n",
    "    if os.path.exists(f'benchmarking_data/data_{size}.jsonl'):\n",
    "        print(f\"File benchmarking_data/data_{size}.jsonl already exists, skipping...\")\n",
    "        continue\n",
    "\n",
    "    if size < len(data):\n",
    "        subset = data[:size]\n",
    "    else:\n",
    "        subset = data + [data[i % len(data)] for i in range(size - len(data))]\n",
    "\n",
    "    assert len(subset) == size, f\"Subset size mismatch: expected {size}, got {len(subset)}\"\n",
    "\n",
    "    with open(f'benchmarking_data/data_{size}.jsonl', 'w') as f:\n",
    "        for item in subset:\n",
    "            f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c82a51",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6365072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/NS/llm-pretraining/work/afkhan/tokensmith\")\n",
    "sys.path.insert(0, \"/NS/llm-pretraining/work/afkhan/USC_Colab/gpt-neox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad27c303",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/NS/venvs/work/afkhan/neox_updated_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "TOKENIZER_NAME_OR_PATH = \"EleutherAI/gpt-neox-20b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME_OR_PATH, add_eos_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96abb13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokensmith import DatasetManager\n",
    "\n",
    "dataset_manager = DatasetManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b42f8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 19303.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files benchmarking_data/data_1_text_document.bin and benchmarking_data/data_1_text_document.idx already exist, skipping...\n",
      "Files benchmarking_data/data_10_text_document.bin and benchmarking_data/data_10_text_document.idx already exist, skipping...\n",
      "Files benchmarking_data/data_100_text_document.bin and benchmarking_data/data_100_text_document.idx already exist, skipping...\n",
      "Files benchmarking_data/data_1000_text_document.bin and benchmarking_data/data_1000_text_document.idx already exist, skipping...\n",
      "Files benchmarking_data/data_10000_text_document.bin and benchmarking_data/data_10000_text_document.idx already exist, skipping...\n",
      "Files benchmarking_data/data_100000_text_document.bin and benchmarking_data/data_100000_text_document.idx already exist, skipping...\n",
      "Files benchmarking_data/data_1000000_text_document.bin and benchmarking_data/data_1000000_text_document.idx already exist, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for size in tqdm(sizes):\n",
    "    path = f'benchmarking_data/data_{size}.jsonl'\n",
    "    output_prefix = f'benchmarking_data/data_{size}'\n",
    "    log_file = f'benchmarking_data/log_{size}.txt'\n",
    "\n",
    "    if os.path.exists(f'{output_prefix}_text_document.bin') and os.path.exists(f'{output_prefix}_text_document.idx'):\n",
    "        print(f\"Files {output_prefix}_text_document.bin and {output_prefix}_text_document.idx already exist, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    dataset_manager.ingest.ingest_from_jsonl(\n",
    "        input_jsonl_path=path,\n",
    "        output_prefix=output_prefix,\n",
    "        vocab_path='/NS/llm-pretraining/work/afkhan/tokensmith/artifacts/tokenizer.json',\n",
    "        neox_dir='/NS/llm-pretraining/work/afkhan/USC_Colab/gpt-neox',\n",
    "        workers=8,\n",
    "        append_eod=True,\n",
    "        dataset_impl='mmap',\n",
    "        tokenizer_type='HFTokenizer',\n",
    "        log_file=log_file,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87aafc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neox_updated_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
