import argparse
import json
import logging
import numpy as np
import os
from tqdm import trange
import uuid
from typing import Optional, List, Dict, Any
from megatron.data.indexed_dataset import MMapIndexedDataset
from megatron.data.data_utils import get_train_valid_test_split_
from megatron.data.gpt2_dataset import _build_index_mappings
import re
from transformers import AutoTokenizer
import torch
import os
from functools import lru_cache
import time

logger = logging.getLogger(__name__)

@lru_cache(1)
def warn_once(logger: logging.Logger, msg: str):
    logger.warning(msg)
    time.sleep(10)

def generate_training_sample(tokenized_segments: List[List[int]], tokenizer: AutoTokenizer) -> str:
    concat_training_sample = np.concat(tokenized_segments)
    return tokenizer.decode(
        concat_training_sample,
    )



class BatchInfo:
    def __init__(self, batch_info_prefix: str):
        self.doc_idx = np.load(f"{batch_info_prefix}_doc_idx.npy", allow_pickle=True, mmap_mode="r")
        self.sample_idx = np.load(f"{batch_info_prefix}_sample_idx.npy", allow_pickle=True, mmap_mode="r")
        self.shuffle_idx = np.load(f"{batch_info_prefix}_shuffle_idx.npy", allow_pickle=True, mmap_mode="r")
    
    def get_example_details_by_id(self, example_loc: int) -> dict:
        pt_shuffle_idx = self.shuffle_idx[example_loc]
        doc_index_f = self.sample_idx[pt_shuffle_idx][0]
        doc_index_l = self.sample_idx[pt_shuffle_idx + 1][0]
        offset_f = self.sample_idx[pt_shuffle_idx][1]
        offset_l = self.sample_idx[pt_shuffle_idx + 1][1]
        return {
            "doc_index_f": doc_index_f,
            "doc_index_l": doc_index_l,
            "offset_f": offset_f,
            "offset_l": offset_l
        }

    def get_doc_index_in_corpus(self, doc_index: int) -> int:
        return self.doc_idx[doc_index]

class WriteableMMapIndexedDataset:
    def __init__(self, 
                 dataset_prefix: str, 
                 batch_info_save_prefix: str, 
                 train_seq_len: int, 
                 train_iters: int,
                 train_batch_size: int,
                 seed: int,
                 splits_string: str,
                 packing_impl: str,
                 allow_chopped: bool,
                 add_extra_token_to_seq: int):
        logger.debug(f"Initializing WriteableMMapIndexedDataset with pointer: {dataset_prefix}.bin and index: {dataset_prefix}.idx")

        batch_info_save_path = f"{batch_info_save_prefix}_train_indexmap_{train_iters*train_batch_size}ns_{train_seq_len}sl_{seed}s_{packing_impl}pi"

        if allow_chopped:
            batch_info_save_path += "_ac"

        logger.debug(f"Loading doc/sample/shuffle indexes with prefix: {batch_info_save_prefix}")
        self.corpus_pointer = open(f"{dataset_prefix}.bin", 'r+b')
        self.corpus_index = MMapIndexedDataset.Index(f"{dataset_prefix}.idx")
        self.corpus_dtype = self.corpus_index.dtype
        if not os.path.exists(f"{batch_info_save_path}_doc_idx.npy"):
            self.simulate_training_run(batch_info_prefix=batch_info_save_prefix,
                                       train_seq_len=train_seq_len,
                                       train_iters=train_iters,
                                       train_batch_size=train_batch_size,
                                       seed=seed,
                                       splits_string=splits_string,
                                       packing_impl=packing_impl,
                                       allow_chopped=allow_chopped
                                       )
        self.batch_info = BatchInfo(batch_info_save_path)
        self.train_seq_len = train_seq_len
        self.add_extra_token_to_seq = add_extra_token_to_seq  # Default to 1 to account adding EOS token
    
    def close(self):
        """
        Closes the corpus pointer to release the file handle.
        """
        logger.debug("Closing corpus pointer.")
        self.corpus_pointer.close()

    def simulate_training_run(self, 
                              batch_info_prefix: str,
                                train_seq_len: int,
                              train_iters: int,
                              train_batch_size: int,
                              seed: int,
                              splits_string: str,
                              packing_impl: str,
                              allow_chopped: bool):
        """
        Simulates a training run by creating doc_idx, sample_idx, and shuffle_idx files for the training sets.
        This is a placeholder method. It is better to use files generated by the training run.
        """
        os.environ['MASTER_ADDR'], orig_master_addr = '127.0.0.1', os.environ.get('MASTER_ADDR', None)
        os.environ['MASTER_PORT'], orig_master_port = '29500', os.environ.get('MASTER_PORT', None)
        
        # torch.distributed.init_process_group(backend='nccl', rank=0, world_size=1)

        import deepspeed
        deepspeed.init_distributed(dist_backend="nccl", auto_mpi_discovery=True,
                                distributed_port="6000", verbose=True)
        from deepspeed.runtime.pipe.topology import PipeModelDataParallelTopology
        topo = PipeModelDataParallelTopology(num_pp=1, num_mp=1, num_dp=1)
        from megatron import mpu
        mpu.initialize_model_parallel(1, topo, fp32_allreduce=True)

        logger.warning("Simulating training run. This is method generates the shuffling and batching order for the training data.")
        
        # splits_string = "969, 30, 1"
        # logger.warning(f"> Using train-val-test splits: {splits_string}")
        total_num_of_documents = self.corpus_index.sizes.shape[0]
        splits = get_train_valid_test_split_(splits_string, total_num_of_documents)
        documents_ = np.arange(start=splits[0], stop=splits[1], step=1, dtype=np.int32)
        
        # Example: "dataset_train_indexmap_100ns_1024sl_1s_0pi_ac"
        # pattern = r"(.*)_train_indexmap_(\d+)ns_(\d+)sl_(\d+)s_(.*)pi_ac"
        # match = re.match(pattern, batch_info_prefix)
        # if not match:
        #     raise ValueError(f"batch_info_prefix '{batch_info_prefix}' does not match expected pattern")
        # data_prefix = match.group(1)
        # num_samples = int(match.group(2))
        # seq_length = int(match.group(3))
        # seed = int(match.group(4))
        # packing_impl = match.group(5)
        # assert packing_impl == 'packed', f"Expected packing implementation to be 'packed', got '{packing_impl}'"

        num_samples = train_iters * train_batch_size
        data_prefix = batch_info_prefix

        
        logger.warning(f"> Data prefix: {data_prefix}, num_samples: {num_samples}, seq_length: {train_seq_len}, seed: {seed}, packing_impl: {packing_impl}")
        _, _, _ = _build_index_mappings(
            'train',
            data_prefix,
            documents_,
            self.corpus_index.sizes,
            None,
            num_samples,
            None,
            train_seq_len,
            seed,
            packing_impl,
            use_shared_fs=True,
            allow_chopped=allow_chopped,
        )

        # torch.distributed.monitored_barrier(
        #     group=torch.distributed.group.WORLD,
        #     timeout=60,
        # )

        # # Timeout
        # torch.distributed.destroy_process_group()

        os.environ['MASTER_ADDR'] = orig_master_addr if orig_master_addr is not None else ''
        os.environ['MASTER_PORT'] = orig_master_port if orig_master_port is not None else ''

    def get_document_by_id(self, doc_index: int) -> np.ndarray:
        """
        Reads a document from the MMapIndexedDataset by its index.
        
        Args:
            doc_index (int): The index of the document to read.
        
        Returns:
            np.ndarray: A numpy array containing the data read from the document.
        """
        pt_byte_offset, size = self.corpus_index[doc_index]
        self.corpus_pointer.seek(pt_byte_offset)
        return np.frombuffer(self.corpus_pointer.read(size * np.dtype(self.corpus_dtype).itemsize),
                             dtype=np.dtype(self.corpus_dtype))

    def get_example_by_id(self, example_loc: int, return_doc_details: bool = False):
        """
        Reads an example from the MMapIndexedDataset by its location in a training run.
        
        Args:
            example_loc (int): The index of the example to read.
            return_doc_details (bool): If True, returns the document details along with the data.
        
        Returns:
            list of np.ndarray: A list of numpy arrays, each containing the data read from the corresponding document segment.
            doc_details (dict, optional): If `return_doc_details` is True, returns a dictionary with document details.
        Notes:
            - If the sequence is contained within a single document, only one array is returned in the list.
            - If the sequence spans multiple documents, the list contains one array per document segment.
            - The dtype used for reading is inferred from `corpus_index_.dtype`.
        """
        output_seq = []

        doc_details = self.batch_info.get_example_details_by_id(example_loc)
        doc_index_f_, doc_index_l_ = doc_details["doc_index_f"], doc_details["doc_index_l"]
        offset_f_, offset_l_ = doc_details["offset_f"], doc_details["offset_l"]
        if doc_index_f_ == doc_index_l_:
            pt_byte_offset, _ = self.corpus_index[self.batch_info.get_doc_index_in_corpus(doc_index_f_)]
            pt_byte_offset += offset_f_ * np.dtype(self.corpus_dtype).itemsize
            item_length = (offset_l_ - offset_f_ + self.add_extra_token_to_seq) * np.dtype(self.corpus_dtype).itemsize
            self.corpus_pointer.seek(pt_byte_offset)
            output_seq.append(np.frombuffer(self.corpus_pointer.read(item_length),
                                            dtype=np.dtype(self.corpus_dtype)))
        else:
            pt_byte_offset, size = self.corpus_index[self.batch_info.get_doc_index_in_corpus(doc_index_f_)]
            pt_byte_offset += offset_f_ * np.dtype(self.corpus_dtype).itemsize
            item_length = (size - offset_f_) * np.dtype(self.corpus_dtype).itemsize
            self.corpus_pointer.seek(pt_byte_offset)
            output_seq.append(np.frombuffer(self.corpus_pointer.read(item_length),
                                            dtype=np.dtype(self.corpus_dtype)))

            for i in range(doc_index_f_ + 1, doc_index_l_):
                pt_byte_offset, size = self.corpus_index[self.batch_info.get_doc_index_in_corpus(i)]
                item_length = size * np.dtype(self.corpus_dtype).itemsize
                self.corpus_pointer.seek(pt_byte_offset)
                output_seq.append(np.frombuffer(self.corpus_pointer.read(item_length),
                                                dtype=np.dtype(self.corpus_dtype)))
            
            pt_byte_offset, size = self.corpus_index[self.batch_info.get_doc_index_in_corpus(doc_index_l_)]
            item_length = (offset_l_ + self.add_extra_token_to_seq) * np.dtype(self.corpus_dtype).itemsize
            self.corpus_pointer.seek(pt_byte_offset)
            output_seq.append(np.frombuffer(self.corpus_pointer.read(item_length),
                                            dtype=np.dtype(self.corpus_dtype)))
        if return_doc_details:
            return output_seq, doc_details
        return output_seq

    def write_example_into_corpus(self, injection_loc: int, injection_data: np.ndarray, dry_run: bool = False):
        """
        Writes an example into the corpus at the specified location (sample number in a training run).
        
        Args:
            injection_loc (int): The index of the example to write.
            injection_data (np.ndarray): The data to write into the corpus.
            dry_run (bool): If True, only simulates the write operation without actually modifying the corpus.
        
        Returns:
            doc_details (dict): A dictionary containing details about the corpus documents where the data was injected.
        """

        if not dry_run:
            warn_once(logger, "This warning will not be shown again. The process will wait for 10 seconds before starting the edit process.")

        doc_details = self.batch_info.get_example_details_by_id(injection_loc)
        doc_index_f_, doc_index_l_ = doc_details["doc_index_f"], doc_details["doc_index_l"]
        offset_f_, offset_l_ = doc_details["offset_f"], doc_details["offset_l"]

        if doc_index_f_ == doc_index_l_:
            pt_byte_offset, _ = self.corpus_index[self.batch_info.get_doc_index_in_corpus(doc_index_f_)]
            pt_byte_offset += offset_f_ * np.dtype(self.corpus_dtype).itemsize
            assert (offset_l_ - offset_f_ + 1) >= len(injection_data)
            logger.debug(f'>>> Inserting {len(injection_data)} tokens starting at position {pt_byte_offset}')
            if not dry_run:
                self.corpus_pointer.seek(pt_byte_offset)
                self.corpus_pointer.write(injection_data.tobytes(order="C"))
        else:
            pt_byte_offset, size = self.corpus_index[self.batch_info.get_doc_index_in_corpus(doc_index_f_)]
            pt_byte_offset += offset_f_ * np.dtype(self.corpus_dtype).itemsize
            ex_space = size - offset_f_
            
            
            logger.debug(f'>>> Inserting {min(ex_space, len(injection_data))} of {len(injection_data)} tokens starting at position {pt_byte_offset}')

            ex_left = len(injection_data) - ex_space
            if not dry_run:
                logger.debug(f'>>> Injection Data Shape {injection_data[:ex_space].shape} | Injection Data Dtype {injection_data[:ex_space].dtype}')
                self.corpus_pointer.seek(pt_byte_offset)
                self.corpus_pointer.write(injection_data[:ex_space].tobytes(order="C"))
            ex_done = ex_space

            if ex_left > 0:
                for i in range(doc_index_f_ + 1, doc_index_l_):
                    pt_byte_offset, size = self.corpus_index[self.batch_info.get_doc_index_in_corpus(i)]
                    ex_space = size
                    
                    logger.debug(f'>>> Inserting {min(ex_space, ex_left)} of {len(injection_data)} tokens starting at position {pt_byte_offset}')
                    ex_left -= ex_space
                    if not dry_run:
                        self.corpus_pointer.seek(pt_byte_offset)
                        self.corpus_pointer.write(injection_data[ex_done:ex_done+ex_space].tobytes(order="C"))
                    ex_done += ex_space
                    if ex_left <= 0:
                        break
            
            if ex_left > 0:
                pt_byte_offset, size = self.corpus_index[self.batch_info.get_doc_index_in_corpus(doc_index_l_)]
                ex_space = offset_l_ + 1
                
                logger.debug(f'>>> Inserting {min(ex_left, ex_space)} of {len(injection_data)} tokens starting at position {pt_byte_offset}')
                ex_left -= ex_space
                if not dry_run:
                    self.corpus_pointer.seek(pt_byte_offset)
                    self.corpus_pointer.write(injection_data[ex_done:ex_done+ex_space].tobytes(order="C"))
                ex_done += ex_space
            
            assert ex_left <= 0
            assert ex_done >= len(injection_data)
        return doc_details
        
    def inject_example_into_corpus(self, injection_loc: int, injection_data: np.ndarray,
                                   injection_type: str, rng: np.random.Generator, 
                                   dry_run: bool = False):
        """
        Injects an example into the corpus at the specified location (sample number in a training run).
        
        Args:
            injection_loc (int): The index of the example to inject.
            injection_data (np.ndarray): The data to inject into the corpus.
            injection_type (str): The type of injection, e.g., "seq_start" or "seq_shuffle".
            rng (np.random.Generator): Random number generator for sampling positions.
            dry_run (bool): If True, only simulates the injection operation without actually modifying the corpus.
        """
        injection_details = {}

        # Cast injection_data to the corpus dtype
        if injection_data.dtype != self.corpus_dtype:
            logger.warning(f">> Casting injection data from {injection_data.dtype} to {self.corpus_dtype}")
            injection_data = injection_data.astype(self.corpus_dtype)

        if injection_type == "seq_shuffle":
            """
            Step 1: Read in the training sequence
            Step 2: Sample an injection position and offset for the new training sequence window
            Step 3: Create the  full sequence
            Step 4: Shorten the sequence to the train_seq_len
            Step 5: Inject it into the tokenized corpus
            """
            # Step 1: Read in the orig training sequence
            pt_train_seqs = self.get_example_by_id(injection_loc)
            pt_train_szs = [len(one_seq) for one_seq in pt_train_seqs]
            assert len(np.concatenate(pt_train_seqs)) == self.train_seq_len + self.add_extra_token_to_seq
            # Step 2: Sample an injection position and offset for the new training sequence window
            injection_pos = rng.choice(len(pt_train_seqs), 1)[0]
            if injection_pos == 0:
                window_offset = 0
            else:
                window_offset = rng.choice(len(injection_data), 1)[0]
            # Step 3: Create the full sequence
            pt_train_seqs.insert(injection_pos, injection_data)
            concat_pt_seq = np.concatenate(pt_train_seqs)
            # Step 4: Shorten the sequence to the train_seq_len
            if injection_pos > 0:
                injection_start = sum(pt_train_szs[:injection_pos])
                injection_end = injection_start + len(injection_data)
                used_check = False
                if injection_start < window_offset:
                    used_check = True
                    logger.debug(f"> Window check: Restricting window_offset ({window_offset}) <= injection_start ({injection_start}).")
                    window_offset = min(window_offset, injection_start)
                if (window_offset + self.train_seq_len + self.add_extra_token_to_seq) < injection_end:
                    used_check = True
                    logger.debug(f"> Window check: Restricting window_offset ({window_offset}) + train_seq_len ({self.train_seq_len + self.add_extra_token_to_seq}) >= injection_end ({injection_end}).")
                    window_offset = max(window_offset, injection_end - self.train_seq_len - 1)
                if used_check:
                    injection_details['check_used'] = 1
            else:
                injection_start = 0
            
            injection_details["pt_injection_pos"] = int(injection_pos)
            injection_details["pt_window_offset"] = int(window_offset)
            injection_details["pt_injection_len"] = len(injection_data)
            injection_details["orig_doc_seq_sizes"] = pt_train_szs


            concat_pt_seq = concat_pt_seq[window_offset:window_offset + self.train_seq_len + self.add_extra_token_to_seq]
            assert (concat_pt_seq[injection_start - window_offset: injection_start - window_offset + len(injection_data)] == injection_data).all()

            # Step 5: Replace perturbation object with the full sequence and 
            # inject it into the tokenized corpus
            injection_data = concat_pt_seq

            if dry_run:
                injection_details['injection_data'] = injection_data

            assert len(injection_data) == self.train_seq_len + self.add_extra_token_to_seq
        else:
            # The injected data will overwrite the existing training sequence
            # Overwtiting will happen at the start of the sequence
            pass

        injection_doc_details = self.write_example_into_corpus(
            injection_loc=injection_loc,
            injection_data=injection_data,
            dry_run=dry_run
        )
        injection_details.update(injection_doc_details)
        return injection_details


def perturb_dataset(raw_dataset: str,
                    batch_info: str,
                    perturbation_dir: str,
                    max_train_samples: int,
                    max_train_batches: int,
                    train_seq_len: int,
                    add_extra_token_to_seq: int,
                    injection_type: str,
                    loc_sampler: str,
                    seed: int,
                    dry_run: bool = False,
                    perturbation_include_filters: Optional[List[str]] = None
                    ) -> None:
    logger.warning(">>>>>>>>>>>>>>>>>>>>>>>>>")
    logger.warning("WARNING: Index sizes will be inconsistent with the actual document boundaries.")
    logger.warning("<<<<<<<<<<<<<<<<<<<<<<<<<")

    perturbation_files = sorted([pfnm[:-len('.bin')] for pfnm in os.listdir(perturbation_dir) if pfnm.endswith('.bin')])
    if perturbation_include_filters is not None:
        logger.info("> Filtering perturbation files")
        perturbation_files = [pfnm for pfnm in perturbation_files if any(incl_filter in pfnm for incl_filter in perturbation_include_filters)]
        logger.info(f">> Including perturbations with: {perturbation_include_filters}")
        logger.info(f">> Perturbations to be used: {perturbation_files}")

    samples_to_insert = 0
    for perturbation_nm in perturbation_files:
        logger.info(f"> Loading: {os.path.split(perturbation_nm)}")
        mmap_perturb_set = MMapIndexedDataset(os.path.join(perturbation_dir, perturbation_nm))
        samples_to_insert += len(mmap_perturb_set)
    assert samples_to_insert <= max_train_samples
    logger.info(f"> Inserting {samples_to_insert} samples.")
    logger.info(f"> Perturbing {samples_to_insert / max_train_samples * 100}% of the training samples.")
    logger.info(f"> Perturbing {samples_to_insert / max_train_batches * 100}% of the training batches (expectation).")

    rng = np.random.default_rng(seed)
    if loc_sampler == "seq":
        perturbation_locs = rng.choice(max_train_samples, samples_to_insert, replace=False)
    elif loc_sampler == "batch":
        assert samples_to_insert < max_train_batches, f"More perturbations ({samples_to_insert}) than batches ({max_train_batches})"
        assert max_train_samples % max_train_batches == 0, "Inferred batch size is not an integer"
        batch_sz = max_train_samples // max_train_batches
        logger.info(f"> Inferred batch size = {batch_sz}")
        perturbation_batches = rng.choice(max_train_batches, samples_to_insert, replace=False)
        perturbation_offsets = rng.choice(batch_sz, samples_to_insert, replace=True)
        perturbation_locs = perturbation_batches * batch_sz + perturbation_offsets
        assert len(set(perturbation_locs.flatten())) == samples_to_insert
        assert perturbation_locs.max() < max_train_samples
    else:
        raise NotImplementedError(f"Unknown loc_sampler: {loc_sampler}")

    writeable_dataset = WriteableMMapIndexedDataset(raw_dataset, batch_info, train_seq_len=train_seq_len, add_extra_token_to_seq=add_extra_token_to_seq)

    p_ctr = 0
    prior_perturbation_locs = set()
    perturbation_info = []
    check_used = 0
    printed_ctr = 0
    for perturbation_nm in perturbation_files:
        logger.info(f'> Begin adding file {perturbation_nm}')
        mmap_perturb_set = MMapIndexedDataset(os.path.join(perturbation_dir, perturbation_nm))
        assert max(mmap_perturb_set._index.sizes) <= train_seq_len

        for idx_ in trange(len(mmap_perturb_set)):
            one_pt_ex = mmap_perturb_set.get(idx_)

            pt_loc = perturbation_locs[p_ctr]
            assert pt_loc not in prior_perturbation_locs
            prior_perturbation_locs.add(pt_loc)
            
            injection_details = {
                "perturbation_file": perturbation_nm,
                "perturbation_idx": idx_,
                "pt_loc": int(pt_loc),
            }
            
            corpus_injection_details = writeable_dataset.inject_example_into_corpus(
                injection_loc=pt_loc,
                injection_data=one_pt_ex,
                injection_type=injection_type,
                rng=rng,
                dry_run=dry_run
            )
            
            injection_details.update(corpus_injection_details)
            perturbation_info.append(injection_details)

            check_used += injection_details.get('check_used', 0)
            p_ctr += 1
            if dry_run and printed_ctr > 10:
                break
    
    rand_suffix = uuid.uuid4().hex[:8]
    out_filename = f"{raw_dataset}_perturbation_info_{rand_suffix}.json"
    logger.info(f"Writing perturbation info to {out_filename}")
    with open(out_filename, 'w') as fout:
        out_str = '\n'.join([json.dumps(one_info) for one_info in perturbation_info]) + '\n'
        fout.write(out_str)

    logger.debug(f"> Window check used {check_used} times.")
    writeable_dataset.close()


# def parse_args():
#     parser = argparse.ArgumentParser()

#     parser.add_argument(
#         '--exp_name',
#         required=True,
#         help="The name of the experiment that will be run. For logging purposes only."
#     )

#     parser.add_argument(
#         '--raw_dataset',
#         required=True,
#         help="the path to the tokenized base dataset"
#     )

#     parser.add_argument(
#         '--batch_info',
#         required=True,
#         help="the path to the shuffled and sampled batch data"
#     )

#     parser.add_argument(
#         '--perturbation_dir',
#         required=True,
#         help="the path to the perturbation data"
#     )

#     parser.add_argument(
#         '--perturbation_include_filters',
#         default=None,
#         nargs='+',
#         help="the list of filename filters (substrings) to include in the perturbation process"
#     )

#     parser.add_argument(
#         '--max_train_samples',
#         required=True,
#         type=int,
#         help="Maximum number of samples that will be used for model training"
#     )

#     parser.add_argument(
#         '--max_train_batches',
#         required=True,
#         type=int,
#         help="Maximum number of batches that will be used for model training"
#     )

#     parser.add_argument(
#         '--train_seq_len',
#         type=int,
#         required=True,
#         # default=1024,
#         help="Sample length of training sequences"
#     )

#     parser.add_argument(
#         '--add_extra_token_to_seq',
#         type=int,
#         default=1,
#         help="Number of extra tokens to add to the training sequence. This is used to account for extra tokens for causal LM pre-training."            
#     )
    
#     parser.add_argument(
#         '--injection_type',
#         choices=["seq_start", "seq_shuffle"],
#         default="seq_start",
#         help="Whether to inject perturbation data at the "
#              "(1) overwrite start of a randomly sampled sequence, "
#              "(2) shuffled into a training sequence (insert into and resize old data instead of overwriting any data)"
#     )

#     # parser.add_argument(
#     #     '--eos_tok_id',
#     #     type=int,
#     #     default=50279,
#     #     help="Token ID of <endoftext>"
#     # )

#     parser.add_argument(
#         '--loc_sampler',
#         choices=["seq", "batch"],
#         default="seq",
#         help="Whether to randomly sample a sequence or batch for injection"
#     )

#     parser.add_argument(
#         '--seed',
#         required=True,
#         type=int,
#         help="the seed to use"
#     )

#     parser.add_argument(
#         '--dry_run',
#         action='store_true',
#         help="Only simulate addition of the perturbation data"
#     )

#     return parser.parse_args()

# if __name__=="__main__":
#     args_ = parse_args()
#     logger.info(f"> Args: {vars(args_)}")
#     perturb_dataset(raw_dataset=args_.raw_dataset,
#                     batch_info=args_.batch_info,
#                     perturbation_dir=args_.perturbation_dir,
#                     max_train_samples=args_.max_train_samples,
#                     max_train_batches=args_.max_train_batches,
#                     train_seq_len=args_.train_seq_len,
#                     add_extra_token_to_seq=args_.add_extra_token_to_seq,
#                     injection_type=args_.injection_type,
#                     loc_sampler=args_.loc_sampler,
#                     seed=args_.seed,
#                     dry_run=args_.dry_run,
#                     perturbation_include_filters=args_.perturbation_include_filters
#                     )