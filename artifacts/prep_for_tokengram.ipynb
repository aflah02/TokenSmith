{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76a48cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'r\\xb5\\x0f\\x00Nn\\xe7\\x0b\\xfd\\x006uY\\x04\\x1d\\x01'\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "\n",
    "with open(\"/NS/llm-pretraining/work/afkhan/tokensmith/artifacts/data_tokenized_text_document.bin\", \"rb\") as f:\n",
    "    header = f.read(16)  # Try the first 16 or 32 bytes\n",
    "    print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8edcc9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = struct.unpack('<QQ', header) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a03cb2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(857775535324902770, 80225150627610877)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6261db82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "\n",
    "def convert_neox_to_tokengrams(bin_path, idx_path, output_bin_path, drop_eod=True, eod_token=50256):\n",
    "    \"\"\"\n",
    "    Converts a Megatron-GPT-NeoX .bin/.idx dataset into a raw .bin format usable by Tokengrams.\n",
    "    \n",
    "    Parameters:\n",
    "    - bin_path: str, path to the original .bin file\n",
    "    - idx_path: str, path to the original .idx file\n",
    "    - output_bin_path: str, where to write the flat token stream\n",
    "    - drop_eod: bool, if True, drops the last token of every sequence if it equals the EOD token\n",
    "    - eod_token: int, token ID that marks end-of-document\n",
    "    \"\"\"\n",
    "    with open(idx_path, 'rb') as f:\n",
    "        magic = f.read(8)\n",
    "        if magic != b'MMIDIDX\\x00':\n",
    "            raise ValueError(\"Only mmap-style indexed dataset supported.\")\n",
    "\n",
    "        version = struct.unpack('<Q', f.read(8))[0]\n",
    "        dtype_code = struct.unpack('<B', f.read(1))[0]\n",
    "        dtype_map = {\n",
    "            1: np.uint8, 2: np.int8, 3: np.int16,\n",
    "            4: np.int32, 5: np.int64, 6: np.float16,\n",
    "            7: np.float32, 8: np.uint16\n",
    "        }\n",
    "        if dtype_code not in dtype_map:\n",
    "            raise ValueError(f\"Unsupported dtype code: {dtype_code}\")\n",
    "        dtype = dtype_map[dtype_code]\n",
    "\n",
    "        num_sequences = struct.unpack('<Q', f.read(8))[0]\n",
    "        num_docs = struct.unpack('<Q', f.read(8))[0]\n",
    "\n",
    "        # Read sequence sizes (uint32) and pointers (uint64)\n",
    "        seq_sizes = np.frombuffer(f.read(num_sequences * 4), dtype=np.uint32)\n",
    "        seq_pointers = np.frombuffer(f.read(num_sequences * 8), dtype=np.uint64)\n",
    "\n",
    "        # Skip document index info (not needed)\n",
    "        doc_index_size = num_docs * 2 * 4  # two uint32s per doc\n",
    "        f.seek(doc_index_size, 1)\n",
    "\n",
    "    # Read full token stream from .bin file\n",
    "    token_data = np.memmap(bin_path, dtype=dtype, mode='r')\n",
    "\n",
    "    # Collect sequences into a flat list\n",
    "    flat_tokens = []\n",
    "    for i in range(num_sequences):\n",
    "        start = seq_pointers[i]\n",
    "        end = start + seq_sizes[i]\n",
    "        seq = token_data[start:end]\n",
    "\n",
    "        if drop_eod and len(seq) > 0 and seq[-1] == eod_token:\n",
    "            seq = seq[:-1]  # Drop final EOD token\n",
    "        flat_tokens.append(seq)\n",
    "\n",
    "    # Concatenate and write to file\n",
    "    flat_array = np.concatenate(flat_tokens)\n",
    "    flat_array.astype(dtype).tofile(output_bin_path)\n",
    "\n",
    "    print(f\"âœ… Wrote {len(flat_array)} tokens to {output_bin_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1f53068",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unsupported dtype code: 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mconvert_neox_to_tokengrams\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/NS/llm-pretraining/work/afkhan/tokensmith/artifacts/data_tokenized_text_document.bin\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/NS/llm-pretraining/work/afkhan/tokensmith/artifacts/data_tokenized_text_document.idx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/NS/llm-pretraining/work/afkhan/tokensmith/artifacts/data_tokenized_text_document_flat.bin\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mconvert_neox_to_tokengrams\u001b[39m\u001b[34m(bin_path, idx_path, output_bin_path, drop_eod, eod_token)\u001b[39m\n\u001b[32m     22\u001b[39m dtype_map = {\n\u001b[32m     23\u001b[39m     \u001b[32m1\u001b[39m: np.uint8, \u001b[32m2\u001b[39m: np.int8, \u001b[32m3\u001b[39m: np.int16,\n\u001b[32m     24\u001b[39m     \u001b[32m4\u001b[39m: np.int32, \u001b[32m5\u001b[39m: np.int64, \u001b[32m6\u001b[39m: np.float16,\n\u001b[32m     25\u001b[39m     \u001b[32m7\u001b[39m: np.float32, \u001b[32m8\u001b[39m: np.uint16\n\u001b[32m     26\u001b[39m }\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dtype_map:\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported dtype code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m dtype = dtype_map[dtype_code]\n\u001b[32m     31\u001b[39m num_sequences = struct.unpack(\u001b[33m'\u001b[39m\u001b[33m<Q\u001b[39m\u001b[33m'\u001b[39m, f.read(\u001b[32m8\u001b[39m))[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: Unsupported dtype code: 0"
     ]
    }
   ],
   "source": [
    "convert_neox_to_tokengrams(\n",
    "    \"/NS/llm-pretraining/work/afkhan/tokensmith/artifacts/data_tokenized_text_document.bin\",\n",
    "    \"/NS/llm-pretraining/work/afkhan/tokensmith/artifacts/data_tokenized_text_document.idx\",\n",
    "    \"/NS/llm-pretraining/work/afkhan/tokensmith/artifacts/data_tokenized_text_document_flat.bin\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35314378",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neox_updated_env",
   "language": "python",
   "name": "neox_updated_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
